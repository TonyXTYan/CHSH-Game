from flask import jsonify, Response
import math
from uncertainties import ufloat, UFloat
import uncertainties.umath as um  # for ufloatâ€‘compatible fabs
from functools import lru_cache
from sqlalchemy.orm import joinedload
from src.config import app, socketio, db
from src.state import state
from src.models.quiz_models import Teams, Answers, PairQuestionRounds, ItemEnum
from src.game_logic import QUESTION_ITEMS, TARGET_COMBO_REPEATS, get_effective_combo_repeats
from flask_socketio import emit
from src.game_logic import start_new_round_for_pair
from time import time
import hashlib
import csv
import io
import logging
import threading
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional, Union, Set
from flask import request
from contextlib import contextmanager
import weakref

# Configure logging
logger = logging.getLogger(__name__)

# Dashboard client activity tracking for keep-alive functionality
dashboard_last_activity: Dict[str, float] = {}

# Per-client preference for teams data streaming (enabled/disabled)
dashboard_teams_streaming: Dict[str, bool] = {}

# Cache configuration and throttling constants
CACHE_SIZE = 1024  # LRU cache size for team calculations
REFRESH_DELAY_QUICK = 0.5  # seconds - maximum refresh rate for team updates and data fetching
REFRESH_DELAY_FULL = 1.0  # seconds - maximum refresh rate for expensive full dashboard updates
MIN_STD_DEV = 1e-10  # Minimum standard deviation to avoid zero uncertainty warnings

# Single lock for all dashboard operations to prevent deadlocks
# This lock protects:
# - Cache operations (LRU cache clearing, throttling state)
# - Dashboard client tracking (activity and streaming preferences)
# - All shared state modifications
_dashboard_lock = threading.RLock()

# Global throttling state for get_all_teams function
_last_refresh_time = 0
_cached_teams_result: Optional[List[Dict[str, Any]]] = None
_cached_teams_is_stale = False  # NEW: Track if cached data is stale but still usable

# Global throttling state for dashboard update functions with differentiated timing
_last_team_update_time = 0
_last_full_update_time = 0
_cached_team_metrics: Optional[Dict[str, int]] = None
_cached_full_metrics: Optional[Dict[str, int]] = None
_cached_team_metrics_is_stale = False  # NEW: Track staleness for team metrics
_cached_full_metrics_is_stale = False  # NEW: Track staleness for full metrics

# Global computation flags to prevent race conditions
_teams_computation_in_progress = False
_team_update_computation_in_progress = False
_full_update_computation_in_progress = False

# --- SELECTIVE CACHE INVALIDATION SYSTEM ---

class SelectiveCache:
    """
    Custom cache that supports selective invalidation by team name.
    Preserves cached results for unchanged teams while allowing targeted invalidation.
    Modified to support "stale but usable" invalidation behavior.
    """
    def __init__(self, maxsize: int = CACHE_SIZE):
        self.maxsize = maxsize
        self._cache: Dict[str, Any] = {}
        self._access_order: List[str] = []  # LRU tracking
        self._stale_keys: Set[str] = set()  # NEW: Track which keys are stale
        self._lock = threading.RLock()
    
    def get(self, key: str, allow_stale: bool = True) -> Optional[Any]:
        """
        Get cached value for key, updating LRU order.
        
        Args:
            key: Cache key to retrieve
            allow_stale: If True, return stale data. If False, return None for stale data.
        """
        with self._lock:
            if key in self._cache:
                # Check if key is stale and if stale data is allowed
                if key in self._stale_keys and not allow_stale:
                    return None
                
                # Move to end (most recently used)
                self._access_order.remove(key)
                self._access_order.append(key)
                return self._cache[key]
            return None
    
    def is_stale(self, key: str) -> bool:
        """Check if a cache key is marked as stale."""
        with self._lock:
            return key in self._stale_keys
    
    def set(self, key: str, value: Any) -> None:
        """Set cached value for key, evicting LRU items if needed."""
        with self._lock:
            if key in self._cache:
                # Update existing, move to end
                self._access_order.remove(key)
            elif len(self._cache) >= self.maxsize:
                # Evict least recently used
                lru_key = self._access_order.pop(0)
                del self._cache[lru_key]
                self._stale_keys.discard(lru_key)  # Remove from stale set too
            
            self._cache[key] = value
            self._access_order.append(key)
            # Remove from stale set when setting fresh data
            self._stale_keys.discard(key)
    
    def invalidate_by_team(self, team_name: str) -> int:
        """
        Mark cache entries for a specific team as stale instead of deleting them.
        This allows throttling logic to still return stale data within REFRESH_DELAY.
        Returns number of entries marked as stale.
        """
        with self._lock:
            stale_count = 0
            
            for key in self._cache.keys():
                # Check if this cache key is for the specified team
                if self._is_team_key(key, team_name):
                    self._stale_keys.add(key)
                    stale_count += 1
            
            return stale_count
    
    def clear_all(self) -> None:
        """Clear all cached entries and stale markers."""
        with self._lock:
            self._cache.clear()
            self._access_order.clear()
            self._stale_keys.clear()
    
    def remove_stale_entries(self) -> int:
        """Actually remove stale entries from cache. Returns count of removed entries."""
        with self._lock:
            removed_count = 0
            keys_to_remove = list(self._stale_keys)
            
            for key in keys_to_remove:
                if key in self._cache:
                    del self._cache[key]
                    self._access_order.remove(key)
                    removed_count += 1
            
            self._stale_keys.clear()
            return removed_count
    
    def _is_team_key(self, cache_key: str, team_name: str) -> bool:
        """
        Check if a cache key belongs to a specific team.
        Uses precise matching to avoid false positives from substring matches.
        """
        # For simple team_name keys (exact match)
        if cache_key == team_name:
            return True
        
        # For function cache keys in format: (arg1, arg2, ...)
        # team_name appears as repr(team_name) which is 'team_name'
        team_name_repr = repr(team_name)
        
        # Check if this is a function cache key starting with (team_name, ...)
        if cache_key.startswith(f"({team_name_repr},") or cache_key == f"({team_name_repr})":
            return True
        
        # Check for team_name as any parameter in the function call
        # Use regex to match team_name_repr as a complete parameter
        import re
        # Pattern matches 'team_name' that is:
        # - after opening paren: ('team_name'
        # - after comma and optional space: , 'team_name' or ,  'team_name'
        # - and followed by comma, closing paren, or end: 'team_name', or 'team_name')
        pattern = rf"(\(|,\s*){re.escape(team_name_repr)}(\s*,|\s*\)|$)"
        return bool(re.search(pattern, cache_key))

# Global selective caches
_hash_cache = SelectiveCache()
_correlation_cache = SelectiveCache()
_success_cache = SelectiveCache()
_classic_stats_cache = SelectiveCache()
_new_stats_cache = SelectiveCache()
_team_process_cache = SelectiveCache()

def _make_cache_key(*args, **kwargs) -> str:
    """Create a consistent cache key from function arguments."""
    key_parts = []
    for arg in args:
        key_parts.append(repr(arg))
    for k, v in sorted(kwargs.items()):
        key_parts.append(f"{k}={repr(v)}")
    return f"({', '.join(key_parts)})"

def selective_cache(cache_instance: SelectiveCache):
    """
    Decorator for selective caching that supports team-specific invalidation.
    """
    def decorator(func):
        def wrapper(*args, **kwargs):
            cache_key = _make_cache_key(*args, **kwargs)
            
            # Try to get from cache
            cached_result = cache_instance.get(cache_key)
            if cached_result is not None:
                return cached_result
            
            # Compute and cache result
            result = func(*args, **kwargs)
            cache_instance.set(cache_key, result)
            return result
        
        # Add cache management methods to function
        wrapper.cache_clear = cache_instance.clear_all
        wrapper.cache_invalidate_team = cache_instance.invalidate_by_team
        wrapper.cache_info = lambda: f"Cache entries: {len(cache_instance._cache)}"
        
        return wrapper
    return decorator

# --- END SELECTIVE CACHE SYSTEM ---

@contextmanager
def _safe_dashboard_operation():
    """
    Context manager for safe dashboard operations with proper error handling.
    Ensures lock is always released even if exceptions occur.
    """
    _dashboard_lock.acquire()
    try:
        yield
    except Exception as e:
        logger.error(f"Error in dashboard operation: {str(e)}", exc_info=True)
        raise
    finally:
        _dashboard_lock.release()

def _atomic_client_update(sid: str, activity_time: Optional[float] = None, 
                         streaming_enabled: Optional[bool] = None, 
                         remove: bool = False) -> None:
    """
    Atomically update dashboard client tracking data.
    Ensures both dictionaries are updated together to prevent inconsistencies.
    
    Args:
        sid: Client session ID
        activity_time: Time to set for last activity (if not None)
        streaming_enabled: Streaming preference to set (if not None)
        remove: If True, remove client from tracking dictionaries
    """
    with _safe_dashboard_operation():
        if remove:
            dashboard_last_activity.pop(sid, None)
            dashboard_teams_streaming.pop(sid, None)
            logger.debug(f"Atomically removed dashboard client data for {sid}")
        else:
            if activity_time is not None:
                dashboard_last_activity[sid] = activity_time
            if streaming_enabled is not None:
                dashboard_teams_streaming[sid] = streaming_enabled
            logger.debug(f"Atomically updated dashboard client data for {sid}")

def _get_team_id_from_name(team_name: str) -> Optional[int]:
    """Helper function to resolve team_name to team_id from state or database."""
    try:
        # First check active teams
        team_info = state.active_teams.get(team_name)
        if team_info and 'team_id' in team_info:
            return team_info['team_id']
        
        # Fall back to database lookup
        team = Teams.query.filter_by(team_name=team_name).first()
        return team.team_id if team else None
    except Exception as e:
        logger.error(f"Error getting team_id for team_name {team_name}: {str(e)}", exc_info=True)
        return None

def _periodic_cleanup_dashboard_clients() -> None:
    """
    Periodic cleanup of stale dashboard client data.
    Removes tracking for clients not in active dashboard_clients set.
    Thread-safe with atomic operations to prevent inconsistencies.
    """
    try:
        with _safe_dashboard_operation():
            # Get current active dashboard clients
            active_clients = set(state.dashboard_clients)
            
            # Clean up tracking dictionaries atomically
            stale_activity_clients = set(dashboard_last_activity.keys()) - active_clients
            stale_streaming_clients = set(dashboard_teams_streaming.keys()) - active_clients
            
            # Remove stale clients atomically
            for sid in stale_activity_clients:
                del dashboard_last_activity[sid]
                
            for sid in stale_streaming_clients:
                del dashboard_teams_streaming[sid]
                
            if stale_activity_clients or stale_streaming_clients:
                logger.info(f"Cleaned up {len(stale_activity_clients)} stale activity clients "
                           f"and {len(stale_streaming_clients)} stale streaming clients")
    except Exception as e:
        logger.error(f"Error in periodic cleanup: {str(e)}", exc_info=True)

@socketio.on('keep_alive')
def on_keep_alive() -> None:
    """Handle dashboard client keep-alive ping to maintain connection tracking."""
    try:
        sid = request.sid  # type: ignore
        if sid in state.dashboard_clients:
            _atomic_client_update(sid, activity_time=time())
            emit('keep_alive_ack', to=sid)  # type: ignore
    except Exception as e:
        logger.error(f"Error in on_keep_alive: {str(e)}", exc_info=True)

@socketio.on('set_teams_streaming')
def on_set_teams_streaming(data: Dict[str, Any]) -> None:
    """Handle dashboard client preference for teams data streaming on/off."""
    try:
        sid = request.sid  # type: ignore
        if sid in state.dashboard_clients:
            enabled = data.get('enabled', False)
            _atomic_client_update(sid, streaming_enabled=enabled)
            logger.info(f"Dashboard client {sid} set teams streaming to: {enabled}")
    except Exception as e:
        logger.error(f"Error in on_set_teams_streaming: {str(e)}", exc_info=True)

@socketio.on('request_teams_update')
def on_request_teams_update() -> None:
    """Handle explicit request for teams data from streaming-enabled clients."""
    try:
        sid = request.sid  # type: ignore
        if sid in state.dashboard_clients and dashboard_teams_streaming.get(sid, False):
            # Send current teams data to this specific client
            emit_dashboard_full_update(client_sid=sid)
            logger.info(f"Sent teams update to dashboard client {sid}")
    except Exception as e:
        logger.error(f"Error in on_request_teams_update: {str(e)}", exc_info=True)

@socketio.on('toggle_game_mode')
def on_toggle_game_mode() -> None:
    """Toggle between 'classic' and 'new' game modes with cache invalidation."""
    try:
        sid = request.sid  # type: ignore
        if sid not in state.dashboard_clients:
            emit('error', {'message': 'Unauthorized: Not a dashboard client'})  # type: ignore
            return

        # Toggle the mode
        new_mode = 'new' if state.game_mode == 'classic' else 'classic'
        state.game_mode = new_mode
        logger.info(f"Game mode toggled to: {new_mode}")
        
        # Clear caches to recalculate with new mode - use force clear since mode affects all calculations
        force_clear_all_caches()
        
        # Notify all clients (players and dashboards) about the mode change
        socketio.emit('game_mode_changed', {'mode': new_mode})
        
        # Trigger dashboard update to recalculate metrics immediately
        emit_dashboard_full_update()
        
    except Exception as e:
        logger.error(f"Error in on_toggle_game_mode: {str(e)}", exc_info=True)
        emit('error', {'message': 'An error occurred while toggling game mode'})  # type: ignore

@socketio.on('change_game_theme')
def on_change_game_theme(data: Dict[str, Any]) -> None:
    """Change the game theme and broadcast to all clients."""
    try:
        sid = request.sid  # type: ignore
        if sid not in state.dashboard_clients:
            emit('error', {'message': 'Unauthorized: Not a dashboard client'})  # type: ignore
            return

        new_theme = data.get('theme')
        if not new_theme or not isinstance(new_theme, str):
            emit('error', {'message': 'Invalid theme specified'})  # type: ignore
            return

        # Validate theme against supported themes
        supported_themes = ['classic', 'food', 'aqmjoe']
        if new_theme not in supported_themes:
            emit('error', {'message': f'Unsupported theme "{new_theme}". Supported themes: {", ".join(supported_themes)}'})  # type: ignore
            return

        # Update theme state
        state.game_theme = new_theme
        logger.info(f"Game theme changed to: {new_theme}")
        
        # Notify all clients (players and dashboards) about the theme change
        socketio.emit('game_theme_changed', {'theme': new_theme})
        
        # Also send mode with theme for complete synchronization
        socketio.emit('game_state_sync', {
            'mode': state.game_mode,
            'theme': state.game_theme
        })
        
    except Exception as e:
        logger.error(f"Error in on_change_game_theme: {str(e)}", exc_info=True)
        emit('error', {'message': 'An error occurred while changing game theme'})  # type: ignore

@selective_cache(_hash_cache)
def compute_team_hashes(team_name: str) -> Tuple[str, str]:
    """Generate unique history hashes for team data consistency checking."""
    try:
        # Get team_id from team_name
        team_id = _get_team_id_from_name(team_name)
        if team_id is None:
            logger.warning(f"Could not find team_id for team_name: {team_name}")
            return "NO_TEAM", "NO_TEAM"
        
        # Get all rounds and answers for this team in chronological order
        rounds = PairQuestionRounds.query.filter_by(team_id=team_id).order_by(PairQuestionRounds.timestamp_initiated).all()
        answers = Answers.query.filter_by(team_id=team_id).order_by(Answers.timestamp).all()

        # Create history string containing both questions and answers
        history = []
        for round in rounds:
            history.append(f"P1:{round.player1_item.value if round.player1_item else 'None'}")
            history.append(f"P2:{round.player2_item.value if round.player2_item else 'None'}")
        for answer in answers:
            history.append(f"A:{answer.assigned_item.value}:{answer.response_value}")
        
        history_str = "|".join(history)

        # logger.debug(f"History for team {team_id}: {history_str}")
        # logger.debug(rounds)
        
        # Generate two different hashes
        hash1 = hashlib.sha256(history_str.encode()).hexdigest()[:8]
        hash2 = hashlib.md5(history_str.encode()).hexdigest()[:8]
        
        return hash1, hash2
    except Exception as e:
        logger.error(f"Error computing team hashes: {str(e)}")
        return "ERROR", "ERROR"

# --- AQM JOE THEME SUPPORT ---

def _aqmjoe_label(item: str, ans: bool) -> str:
    """
    Convert boolean answer to AQM Joe theme label.
    
    Args:
        item: Question item ('A', 'B', 'X', 'Y')
        ans: Boolean answer (True/False)
        
    Returns:
        String label ('Green', 'Red', 'Peas', 'Carrots')
    """
    if item in ('A', 'B'):  # Color questions
        return 'Green' if ans else 'Red'
    else:  # Food questions ('X', 'Y')
        return 'Peas' if ans else 'Carrots'


def _is_aqmjoe_success(p1_item: str, p2_item: str, p1_bool: bool, p2_bool: bool) -> bool:
    """
    Determine if a round is successful according to AQM Joe theme rules.
    
    Rules:
    1. Food-Food: success if NOT both "Peas"
    2. Mixed Color-Food: if Color is "Green", Food must be "Peas" to succeed.
       Optional symmetry: if Color is "Red", Food should be "Carrots" to succeed.
    3. Color-Color: neutral for success rate (always successful)
    
    Args:
        p1_item: Player 1's item ('A', 'B', 'X', 'Y')
        p2_item: Player 2's item ('A', 'B', 'X', 'Y')
        p1_bool: Player 1's boolean answer
        p2_bool: Player 2's boolean answer
        
    Returns:
        Boolean indicating if the round is successful
    """
    l1 = _aqmjoe_label(p1_item, p1_bool)
    l2 = _aqmjoe_label(p2_item, p2_bool)
    
    p1_is_color = p1_item in ('A', 'B')
    p2_is_color = p2_item in ('A', 'B')
    p1_is_food = not p1_is_color
    p2_is_food = not p2_is_color
    
    # Rule 3: Food-Food combinations - success if NOT both "Peas"
    if p1_is_food and p2_is_food:
        return not (l1 == 'Peas' and l2 == 'Peas')
    
    # Rule 1: Mixed Color-Food combinations
    if p1_is_color and p2_is_food:
        if l1 == 'Green':
            return l2 == 'Peas'
        # Optional symmetry: Red â†’ Carrots
        return l2 == 'Carrots'
    
    if p2_is_color and p1_is_food:
        if l2 == 'Green':
            return l1 == 'Peas'
        # Optional symmetry: Red â†’ Carrots  
        return l1 == 'Carrots'
    
    # Rule 2: Color-Color combinations - neutral for success rate (always successful)
    return True


@selective_cache(_success_cache)
def compute_success_metrics(team_name: str) -> Tuple[List[List[Tuple[int, int]]], List[str], float, float, Dict[Tuple[str, str], int], Dict[Tuple[str, str], int], Dict[str, Dict[str, int]]]:
    """
    Compute success metrics for new mode instead of correlation matrix.
    Returns success rate matrix, overall success metrics, and individual player balance data.
    """
    try:
        # Get team_id from team_name
        team_id = _get_team_id_from_name(team_name)
        if team_id is None:
            logger.warning(f"Could not find team_id for team_name: {team_name}")
            return ([[(0, 0) for _ in range(4)] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, 0.0, {}, {}, {})
        
        # Get team data for session ID mapping
        db_team = Teams.query.filter_by(team_name=team_name).first()
        if not db_team:
            logger.warning(f"Could not find team data for team_name: {team_name}")
            return ([[(0, 0) for _ in range(4)] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, 0.0, {}, {}, {})
        
        # Get all rounds and their corresponding answers for this team
        rounds = PairQuestionRounds.query.filter_by(team_id=team_id).order_by(PairQuestionRounds.timestamp_initiated).all()
        round_map = {round.round_id: round for round in rounds}
        answers = Answers.query.filter_by(team_id=team_id).order_by(Answers.timestamp).all()
        
        # Group answers by round_id
        answers_by_round: Dict[int, List[Any]] = {}
        for answer in answers:
            if answer.question_round_id not in answers_by_round:
                answers_by_round[answer.question_round_id] = []
            answers_by_round[answer.question_round_id].append(answer)
        
        # Initialize success metrics
        item_values = ['A', 'B', 'X', 'Y']
        success_matrix = [[(0, 0) for _ in range(4)] for _ in range(4)]  # (successful_rounds, total_rounds)
        
        # Count pairs for each item combination
        pair_counts: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        success_counts: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        
        # Track individual player responses for balance calculation
        # NEW MODE: Track each player's responses to their assigned question types
        player_responses: Dict[str, Dict[str, int]] = {}  # {item: {'true': count, 'false': count}}
        for item in item_values:
            player_responses[item] = {'true': 0, 'false': 0}
        
        total_rounds = 0
        successful_rounds = 0
        
        # Analyze each round that has both player answers
        for round_id, round_answers in answers_by_round.items():
            # Skip if we don't have exactly 2 answers (one from each player)
            if len(round_answers) != 2 or round_id not in round_map:
                continue
                
            round_obj = round_map[round_id]
            p1_item = round_obj.player1_item.value if round_obj.player1_item else None
            p2_item = round_obj.player2_item.value if round_obj.player2_item else None
            
            # Skip if we don't have both items
            if not p1_item or not p2_item:
                continue
                
            # Get player responses using session ID matching to avoid duplicate item bug
            p1_answer = None
            p2_answer = None

            # Match answers by player session ID instead of assigned item to handle duplicate items correctly
            for answer in round_answers:
                if answer.player_session_id == db_team.player1_session_id:
                    p1_answer = answer.response_value
                elif answer.player_session_id == db_team.player2_session_id:
                    p2_answer = answer.response_value
            
            # Skip if we don't have both answers
            if p1_answer is None or p2_answer is None:
                continue
                
            # Track individual player responses for balance calculation
            player_responses[p1_item]['true' if p1_answer else 'false'] += 1
            player_responses[p2_item]['true' if p2_answer else 'false'] += 1
                
            # Apply success rules for new mode
            # Success Rule: {B,Y} combinations require different answers; all others require same answers
            is_by_combination = (p1_item == 'B' and p2_item == 'Y') or (p1_item == 'Y' and p2_item == 'B')
            players_answered_differently = p1_answer != p2_answer
            
            if is_by_combination:
                # B,Y combination: players should answer differently
                is_successful = players_answered_differently
            else:
                # All other combinations: players should answer the same
                is_successful = not players_answered_differently
            
            # Update counts
            total_rounds += 1
            if is_successful:
                successful_rounds += 1
                success_counts[(p1_item, p2_item)] += 1
            
            pair_counts[(p1_item, p2_item)] += 1
        
        # Populate the success matrix with (successful, total) tuples
        for i, row_item in enumerate(item_values):
            for j, col_item in enumerate(item_values):
                successful = success_counts.get((row_item, col_item), 0)
                total = pair_counts.get((row_item, col_item), 0)
                success_matrix[i][j] = (successful, total)
        
        # Calculate overall success rate and normalized score
        overall_success_rate = successful_rounds / total_rounds if total_rounds > 0 else 0.0
        
        # Normalized cumulative score: +1 for success, -1 for failure, divided by total rounds
        score_sum = successful_rounds - (total_rounds - successful_rounds)  # successful - failed
        normalized_cumulative_score = score_sum / total_rounds if total_rounds > 0 else 0.0
        
        return (success_matrix, item_values, overall_success_rate, normalized_cumulative_score, success_counts, pair_counts, player_responses)
        
    except Exception as e:
        logger.error(f"Error computing success metrics: {str(e)}", exc_info=True)
        return ([[(0, 0) for _ in range(4)] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, 0.0, {}, {}, {})

@selective_cache(_correlation_cache)
def compute_correlation_matrix(team_name: str) -> Tuple[List[List[Tuple[int, int]]], List[str], float, Dict[str, float], Dict[str, Dict[str, int]], Dict[Tuple[str, str], int], Dict[Tuple[str, str], int]]:
    try:
        # Get team_id from team_name
        team_id = _get_team_id_from_name(team_name)
        if team_id is None:
            logger.warning(f"Could not find team_id for team_name: {team_name}")
            return ([[ (0,0) for _ in range(4) ] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, {}, {}, {}, {})
        
        # Get team data for session ID mapping
        db_team = Teams.query.filter_by(team_name=team_name).first()
        if not db_team:
            logger.warning(f"Could not find team data for team_name: {team_name}")
            return ([[ (0,0) for _ in range(4) ] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, {}, {}, {}, {})
        
        # Get all rounds and their corresponding answers for this team
        # Use separate optimized queries with proper indexing
        rounds = PairQuestionRounds.query.filter_by(team_id=team_id).order_by(PairQuestionRounds.timestamp_initiated).all()
        
        # Create a mapping from round_id to the round object for quick access
        round_map = {round.round_id: round for round in rounds}
        
        # Get all answers for this team with optimized query
        answers = Answers.query.filter_by(team_id=team_id).order_by(Answers.timestamp).all()
        
        # Group answers by round_id
        answers_by_round: Dict[int, List[Any]] = {}
        for answer in answers:
            if answer.question_round_id not in answers_by_round:
                answers_by_round[answer.question_round_id] = []
            answers_by_round[answer.question_round_id].append(answer)
        
        # Prepare the 4x4 correlation matrix for A, B, X, Y combinations
        item_values = ['A', 'B', 'X', 'Y']
        # corr_matrix will store (numerator, denominator) tuples
        corr_matrix = [[(0, 0) for _ in range(4)] for _ in range(4)]
        
        # Count pairs for each item combination
        pair_counts: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        correlation_sums: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        
        # Track same-item responses for the new metric
        same_item_responses: Dict[str, Dict[str, int]] = {}  # Will track {item: {'true': count, 'false': count}}
        
        # Analyze each round that has both player answers
        for round_id, round_answers in answers_by_round.items():
            # Skip if we don't have exactly 2 answers (one from each player)
            if len(round_answers) != 2 or round_id not in round_map:
                continue
                
            round_obj = round_map[round_id]
            p1_item = round_obj.player1_item.value if round_obj.player1_item else None
            p2_item = round_obj.player2_item.value if round_obj.player2_item else None
            
            # Skip if we don't have both items
            if not p1_item or not p2_item:
                continue
                
            # Get player responses using session ID matching to avoid duplicate item bug
            p1_answer = None
            p2_answer = None

            # Match answers by player session ID instead of assigned item to handle duplicate items correctly
            for answer in round_answers:
                if answer.player_session_id == db_team.player1_session_id:
                    p1_answer = answer.response_value
                elif answer.player_session_id == db_team.player2_session_id:
                    p2_answer = answer.response_value
            
            # Track responses for same-item balance metric only when both players have same item
            if p1_item == p2_item and p1_answer is not None and p2_answer is not None:
                if p1_item not in same_item_responses:
                    same_item_responses[p1_item] = {'true': 0, 'false': 0}
                
                # Count each response separately
                same_item_responses[p1_item]['true' if p1_answer else 'false'] += 1
                same_item_responses[p1_item]['true' if p2_answer else 'false'] += 1
            
            # Skip if we don't have both answers (e.g., due to data inconsistency)
            if p1_answer is None or p2_answer is None:
                continue
                
            # Update counts
            # p1_idx = item_values.index(p1_item) # Not needed here anymore
            # p2_idx = item_values.index(p2_item) # Not needed here anymore
            
            # Calculate correlation: (T,T) or (F,F) count as 1, (T,F) or (F,T) count as -1
            correlation = 1 if p1_answer == p2_answer else -1
            
            pair_counts[(p1_item, p2_item)] += 1
            correlation_sums[(p1_item, p2_item)] += correlation
        
        # Populate the corr_matrix with (numerator, denominator) tuples
        for i, row_item in enumerate(item_values):
            for j, col_item in enumerate(item_values):
                numerator = correlation_sums.get((row_item, col_item), 0)
                denominator = pair_counts.get((row_item, col_item), 0)
                corr_matrix[i][j] = (numerator, denominator)
        
        # Calculate the same-item balance metric
        same_item_balance: Dict[str, float] = {}
        for item, counts in same_item_responses.items():
            total = counts['true'] + counts['false']
            if total == 0:
                same_item_balance[item] = 0.0
            else:
                diff = abs(counts['true'] - counts['false'])
                same_item_balance[item] = 1.0 - diff / total
        
        # Calculate the average balance across all same items
        if same_item_responses:
            avg_same_item_balance = sum(same_item_balance.values()) / len(same_item_balance)
        else:
            avg_same_item_balance = 0.0  # Default if no same-item responses
        
        return (corr_matrix, item_values,
                avg_same_item_balance, same_item_balance, same_item_responses,
                correlation_sums, pair_counts)
    except Exception as e:
        logger.error(f"Error computing correlation matrix: {str(e)}", exc_info=True)
        return ([[ (0,0) for _ in range(4) ] for _ in range(4)],
                ['A', 'B', 'X', 'Y'], 0.0, {}, {}, {}, {})

def compute_correlation_stats(team_name: str) -> Tuple[float, float, float]: # NOT USED
    try:
        # Get the correlation matrix and new metrics  
        result = compute_correlation_matrix(team_name)  # type: ignore
        corr_matrix, item_values = result[0], result[1]
        same_item_balance_avg = result[2]
        
        # Validate matrix dimensions and contents
        if not all(isinstance(row, list) and len(row) == 4 for row in corr_matrix) or len(corr_matrix) != 4:
            logger.error(f"Invalid correlation matrix dimensions for team_name {team_name}")
            return 0.0, 0.0, 0.0
            
        # Validate expected item values
        expected_items = ['A', 'B', 'X', 'Y']
        if not all(item in item_values for item in expected_items):
            logger.error(f"Missing expected items in correlation matrix for team_name {team_name}")
            return 0.0, 0.0, 0.0
            
        # Calculate first statistic: Trace(corr_matrix) / 4
        try:
            trace_sum = sum(corr_matrix[i][i] for i in range(4))
            trace_average_statistic = trace_sum / 4
        except (TypeError, IndexError) as e:
            logger.error(f"Error calculating trace statistic: {e}")
            trace_average_statistic = 0.0
        
        # Calculate second statistic using CHSH game formula
        # corrAX + corrAY + corrBX - corrBY + corrXA + corrXB + corrYA - corrYB
        # Get indices for A, B, X, Y from item_values
        try:
            A_idx = item_values.index('A')
            B_idx = item_values.index('B')
            X_idx = item_values.index('X')
            Y_idx = item_values.index('Y')
            
            chsh_value_statistic = (
                corr_matrix[A_idx][X_idx] + corr_matrix[A_idx][Y_idx] + 
                corr_matrix[B_idx][X_idx] - corr_matrix[B_idx][Y_idx] +
                corr_matrix[X_idx][A_idx] + corr_matrix[X_idx][B_idx] + 
                corr_matrix[Y_idx][A_idx] - corr_matrix[Y_idx][B_idx]
            )/2
        except (ValueError, IndexError, TypeError) as e:
            logger.error(f"Error calculating CHSH statistic: {e}")
            chsh_value_statistic = 0.0
        
        return trace_average_statistic, chsh_value_statistic, same_item_balance_avg
    except Exception as e:
        logger.error(f"Error computing correlation statistics: {str(e)}", exc_info=True)
        return 0.0, 0.0, 0.0


@selective_cache(_classic_stats_cache)
def _calculate_team_statistics(team_name: str) -> Dict[str, Optional[float]]:
    """Calculate ufloat statistics from correlation matrix for the given team."""
    try:
        # Get correlation matrix data for this team
        correlation_result = compute_correlation_matrix(team_name)
        (corr_matrix_tuples, item_values,
         same_item_balance_avg, same_item_balance, same_item_responses,
         correlation_sums, pair_counts) = correlation_result
        
        # --- Calculate statistics with uncertainties using ufloat ---

        # Stat1: Trace Average Statistic
        sum_of_cii_ufloats: UFloat = ufloat(0, MIN_STD_DEV)  # Use small non-zero std_dev to avoid warning
        if len(corr_matrix_tuples) == 4 and all(len(row) == 4 for row in corr_matrix_tuples):
            for i in range(4):
                num, den = corr_matrix_tuples[i][i]
                if den > 0:
                    c_ii_val = num / den
                    c_ii_val = max(-1.0, min(1.0, c_ii_val))  # Clamp to valid range
                    stdev_ii = 1 / math.sqrt(den)            # Ïƒ = 1/âˆšN
                    c_ii_ufloat = ufloat(c_ii_val, stdev_ii)
                else:
                    # No statistics â†’ infinite uncertainty
                    c_ii_ufloat = ufloat(0, float("inf"))
                sum_of_cii_ufloats += c_ii_ufloat
        # Average of the four diagonal correlations
        raw_trace_avg_ufloat = (1 / 4) * sum_of_cii_ufloats
        # Force the magnitude to be positive
        # Handle absolute value without using abs() or um.fabs() which are deprecated
        if raw_trace_avg_ufloat.nominal_value >= 0:
            trace_average_statistic_ufloat = raw_trace_avg_ufloat
        else:
            # Create a new ufloat with positive nominal value but same std_dev
            trace_average_statistic_ufloat = ufloat(-raw_trace_avg_ufloat.nominal_value, raw_trace_avg_ufloat.std_dev)

        # Stat2: CHSH Value Statistic
        chsh_sum_ufloat: UFloat = ufloat(0, MIN_STD_DEV)  # Use small non-zero std_dev to avoid warning
        if len(corr_matrix_tuples) == 4 and all(len(row) == 4 for row in corr_matrix_tuples) and all(item in item_values for item in ['A', 'B', 'X', 'Y']):
            try:
                A_idx = item_values.index('A')
                B_idx = item_values.index('B')
                X_idx = item_values.index('X')
                Y_idx = item_values.index('Y')

                terms_indices_coeffs = [
                    (A_idx, X_idx, 1), (A_idx, Y_idx, 1),
                    (B_idx, X_idx, 1), (B_idx, Y_idx, -1),
                    (X_idx, A_idx, 1), (X_idx, B_idx, 1),
                    (Y_idx, A_idx, 1), (Y_idx, B_idx, -1)
                ]

                for r_idx, c_idx, coeff in terms_indices_coeffs:
                    num, den = corr_matrix_tuples[r_idx][c_idx]
                    if den > 0:
                        c_ij_val = num / den
                        c_ij_val = max(-1.0, min(1.0, c_ij_val))
                        stdev_ij = 1 / math.sqrt(den)      # Ïƒ = 1/âˆšN
                        c_ij_ufloat = ufloat(c_ij_val, stdev_ij)
                    else:
                        c_ij_ufloat = ufloat(0, float("inf"))
                    chsh_sum_ufloat += coeff * c_ij_ufloat
            except ValueError:
                pass  # Already handled by the outer condition check
        chsh_value_statistic_ufloat = (1/2) * chsh_sum_ufloat

        # Stat3: Cross-Term Combination Statistic
        cross_term_sum_ufloat: UFloat = ufloat(0, MIN_STD_DEV)  # Use small non-zero std_dev to avoid warning
        # Ensure item_values contains A,B,X,Y before proceeding
        if all(item in item_values for item in ['A', 'B', 'X', 'Y']):
            term_item_pairs_coeffs = [
                ('A', 'X', 1), ('A', 'Y', 1),
                ('B', 'X', 1), ('B', 'Y', -1)
            ]
            for item1, item2, coeff in term_item_pairs_coeffs:
                M_ij = pair_counts.get((item1, item2), 0) + pair_counts.get((item2, item1), 0)
                if M_ij > 0:
                    N_ij_sum_prod = correlation_sums.get((item1, item2), 0) + correlation_sums.get((item2, item1), 0)
                    t_ij_val = N_ij_sum_prod / M_ij
                    t_ij_val = max(-1.0, min(1.0, t_ij_val))
                    stdev_t_ij = 1 / math.sqrt(M_ij)       # Ïƒ = 1/âˆšN
                    t_ij_ufloat = ufloat(t_ij_val, stdev_t_ij)
                else:
                    t_ij_ufloat = ufloat(0, float("inf"))
                cross_term_sum_ufloat += coeff * t_ij_ufloat
        cross_term_combination_statistic_ufloat = cross_term_sum_ufloat

        # --- Sameâ€‘item balance with uncertainty (ufloat) ---
        same_item_balance_ufloats: List[UFloat] = []
        for item, counts in (same_item_responses or {}).items():
            T_count = counts.get('true', 0)
            F_count = counts.get('false', 0)
            total_tf = T_count + F_count
            if total_tf > 0:
                p_val = T_count / total_tf
                var_p = 1 / total_tf  # simplified variance 1/N
                p_true = ufloat(p_val, math.sqrt(var_p))
                p_val2 = 2 * p_true - 1
                if p_val2.nominal_value >= 0:
                    abs_p_val2 = p_val2
                else:
                    abs_p_val2 = ufloat(-p_val2.nominal_value, p_val2.std_dev)
                balance_ufloat = 1 - abs_p_val2
                same_item_balance_ufloats.append(balance_ufloat)
            else:
                # Not enough statistics â€“ propagate infinite uncertainty
                balance_ufloat = ufloat(0, float("inf"))
                same_item_balance_ufloats.append(balance_ufloat)

        if same_item_balance_ufloats:
            avg_same_item_balance_ufloat = sum(same_item_balance_ufloats) / len(same_item_balance_ufloats)
        else:
            avg_same_item_balance_ufloat = ufloat(0, float("inf"))
        
        return {
            'trace_average_statistic': trace_average_statistic_ufloat.nominal_value,
            'trace_average_statistic_uncertainty': trace_average_statistic_ufloat.std_dev if not math.isinf(trace_average_statistic_ufloat.std_dev) else None,
            'chsh_value_statistic': chsh_value_statistic_ufloat.nominal_value,
            'chsh_value_statistic_uncertainty': chsh_value_statistic_ufloat.std_dev if not math.isinf(chsh_value_statistic_ufloat.std_dev) else None,
            'cross_term_combination_statistic': cross_term_combination_statistic_ufloat.nominal_value,
            'cross_term_combination_statistic_uncertainty': cross_term_combination_statistic_ufloat.std_dev if not math.isinf(cross_term_combination_statistic_ufloat.std_dev) else None,
            'same_item_balance': avg_same_item_balance_ufloat.nominal_value,
            'same_item_balance_uncertainty': (avg_same_item_balance_ufloat.std_dev
                                              if not math.isinf(avg_same_item_balance_ufloat.std_dev) else None)
        }
    except Exception as e:
        logger.error(f"Error calculating team statistics: {str(e)}", exc_info=True)
        return {
            'trace_average_statistic': 0.0,
            'trace_average_statistic_uncertainty': None,
            'chsh_value_statistic': 0.0,
            'chsh_value_statistic_uncertainty': None,
            'cross_term_combination_statistic': 0.0,
            'cross_term_combination_statistic_uncertainty': None,
            'same_item_balance': 0.0,
            'same_item_balance_uncertainty': None
        }

@selective_cache(_new_stats_cache)
def _calculate_success_statistics(team_name: str) -> Dict[str, Optional[float]]:
    """Calculate success statistics for new mode from success metrics for the given team."""
    try:
        # Get success metrics data for this team
        success_result = compute_success_metrics(team_name)
        (success_matrix_tuples, item_values, overall_success_rate, normalized_cumulative_score, 
         success_counts, pair_counts, player_responses) = success_result
        
        # Calculate success statistics with uncertainties using ufloat
        # Replace trace_average_statistic with overall_success_rate
        if overall_success_rate >= 0:
            # Calculate uncertainty based on total number of rounds
            total_rounds = sum(pair_counts.values())
            if total_rounds > 0:
                success_rate_uncertainty = math.sqrt(overall_success_rate * (1 - overall_success_rate) / total_rounds)
            else:
                success_rate_uncertainty = None
        else:
            success_rate_uncertainty = None
            
        # Replace chsh_value_statistic with normalized_cumulative_score  
        if normalized_cumulative_score is not None:
            total_rounds = sum(pair_counts.values())
            if total_rounds > 0:
                # Uncertainty for normalized score based on binomial distribution
                score_uncertainty = 2 / math.sqrt(total_rounds)  # Conservative estimate
            else:
                score_uncertainty = None
        else:
            score_uncertainty = None
            
        # Calculate cross-term combination statistic as average success rate across specific pairs
        cross_term_pairs = [('A', 'X'), ('A', 'Y'), ('B', 'X'), ('B', 'Y')]
        cross_term_success_rates = []
        cross_term_uncertainties = []
        
        for item1, item2 in cross_term_pairs:
            total_pair = pair_counts.get((item1, item2), 0) + pair_counts.get((item2, item1), 0)
            successful_pair = success_counts.get((item1, item2), 0) + success_counts.get((item2, item1), 0)
            
            if total_pair > 0:
                pair_success_rate = successful_pair / total_pair
                pair_uncertainty = math.sqrt(pair_success_rate * (1 - pair_success_rate) / total_pair)
                cross_term_success_rates.append(pair_success_rate)
                cross_term_uncertainties.append(pair_uncertainty)
        
        if cross_term_success_rates:
            cross_term_avg = sum(cross_term_success_rates) / len(cross_term_success_rates)
            # Propagate uncertainty (simplified)
            if cross_term_uncertainties:
                cross_term_uncertainty = math.sqrt(sum(u**2 for u in cross_term_uncertainties)) / len(cross_term_uncertainties)
            else:
                cross_term_uncertainty = None
        else:
            cross_term_avg = 0.0
            cross_term_uncertainty = None
            
        # Calculate individual player balance for NEW mode
        # Balance measures how evenly each player distributes True/False answers for their question types
        individual_balances = []
        
        for item, responses in player_responses.items():
            true_count = responses.get('true', 0)
            false_count = responses.get('false', 0)
            total_responses = true_count + false_count
            
            if total_responses > 0:
                # Calculate balance: 1.0 if perfectly balanced (50/50), 0.0 if all same answer
                balance_ratio = min(true_count, false_count) / total_responses
                # Scale to 0-1 range where 1 is perfect balance (50/50)
                item_balance = balance_ratio * 2  # multiply by 2 to make 0.5 ratio = 1.0 balance
                individual_balances.append(item_balance)
        
        if individual_balances:
            same_item_balance_avg = sum(individual_balances) / len(individual_balances)
            # Simple uncertainty calculation based on variance
            if len(individual_balances) > 1:
                variance = sum((b - same_item_balance_avg)**2 for b in individual_balances) / len(individual_balances)
                same_item_balance_uncertainty = math.sqrt(variance)
            else:
                same_item_balance_uncertainty = None
        else:
            same_item_balance_avg = 0.0
            same_item_balance_uncertainty = None
        
        return {
            'trace_average_statistic': overall_success_rate,  # Replace with overall success rate
            'trace_average_statistic_uncertainty': success_rate_uncertainty,
            'chsh_value_statistic': normalized_cumulative_score,  # Replace with normalized score
            'chsh_value_statistic_uncertainty': score_uncertainty,
            'cross_term_combination_statistic': cross_term_avg,
            'cross_term_combination_statistic_uncertainty': cross_term_uncertainty,
            'same_item_balance': same_item_balance_avg,
            'same_item_balance_uncertainty': same_item_balance_uncertainty
        }
    except Exception as e:
        logger.error(f"Error calculating success statistics: {str(e)}", exc_info=True)
        return {
            'trace_average_statistic': 0.0,
            'trace_average_statistic_uncertainty': None,
            'chsh_value_statistic': 0.0,
            'chsh_value_statistic_uncertainty': None,
            'cross_term_combination_statistic': 0.0,
            'cross_term_combination_statistic_uncertainty': None,
            'same_item_balance': 0.0,
            'same_item_balance_uncertainty': None
        }

def _compute_team_hashes_optimized(team_id: int, team_rounds: List[Any], team_answers: List[Any]) -> Tuple[str, str]:
    """Generate unique history hashes for team data consistency checking using pre-fetched data."""
    try:
        # Create history string containing both questions and answers using pre-fetched data
        history = []
        for round_obj in team_rounds:
            history.append(f"P1:{round_obj.player1_item.value if round_obj.player1_item else 'None'}")
            history.append(f"P2:{round_obj.player2_item.value if round_obj.player2_item else 'None'}")
        for answer in team_answers:
            history.append(f"A:{answer.assigned_item.value}:{answer.response_value}")
        
        history_str = "|".join(history)
        
        # Generate two different hashes
        hash1 = hashlib.sha256(history_str.encode()).hexdigest()[:8]
        hash2 = hashlib.md5(history_str.encode()).hexdigest()[:8]
        
        return hash1, hash2
    except Exception as e:
        logger.error(f"Error computing team hashes for team {team_id}: {str(e)}")
        return "ERROR", "ERROR"

def _compute_correlation_matrix_optimized(team_id: int, team_rounds: List[Any], team_answers: List[Any], team_obj: Any = None) -> Tuple[List[List[Tuple[int, int]]], List[str], float, Dict[str, float], Dict[str, Dict[str, int]], Dict[Tuple[str, str], int], Dict[Tuple[str, str], int]]:
    """Compute correlation matrix using pre-fetched rounds and answers data."""
    try:
        # Use pre-fetched team data to avoid N+1 queries
        if team_obj is None:
            # Fallback to database query if team_obj not provided (backward compatibility)
            team_obj = Teams.query.get(team_id)
            if not team_obj:
                logger.warning(f"Could not find team data for team_id: {team_id}")
                return ([[ (0,0) for _ in range(4) ] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, {}, {}, {}, {})
        
        db_team = team_obj
        
        # Create a mapping from round_id to the round object for quick access
        round_map = {round_obj.round_id: round_obj for round_obj in team_rounds}
        
        # Group answers by round_id
        answers_by_round: Dict[int, List[Any]] = {}
        for answer in team_answers:
            if answer.question_round_id not in answers_by_round:
                answers_by_round[answer.question_round_id] = []
            answers_by_round[answer.question_round_id].append(answer)
        
        # Prepare the 4x4 correlation matrix for A, B, X, Y combinations
        item_values = ['A', 'B', 'X', 'Y']
        # corr_matrix will store (numerator, denominator) tuples
        corr_matrix = [[(0, 0) for _ in range(4)] for _ in range(4)]
        
        # Count pairs for each item combination
        pair_counts: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        correlation_sums: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        
        # Track same-item responses for the new metric
        same_item_responses: Dict[str, Dict[str, int]] = {}
        
        # Analyze each round that has both player answers
        for round_id, round_answers in answers_by_round.items():
            # Skip if we don't have exactly 2 answers (one from each player)
            if len(round_answers) != 2 or round_id not in round_map:
                continue
                
            round_obj = round_map[round_id]
            p1_item = round_obj.player1_item.value if round_obj.player1_item else None
            p2_item = round_obj.player2_item.value if round_obj.player2_item else None
            
            # Skip if we don't have both items
            if not p1_item or not p2_item:
                continue
                
            # Get player responses using session ID matching to avoid duplicate item bug
            p1_answer = None
            p2_answer = None

            # Match answers by player session ID instead of assigned item to handle duplicate items correctly
            for answer in round_answers:
                if answer.player_session_id == db_team.player1_session_id:
                    p1_answer = answer.response_value
                elif answer.player_session_id == db_team.player2_session_id:
                    p2_answer = answer.response_value
            
            # Track responses for same-item balance metric only when both players have same item
            if p1_item == p2_item and p1_answer is not None and p2_answer is not None:
                if p1_item not in same_item_responses:
                    same_item_responses[p1_item] = {'true': 0, 'false': 0}
                
                # Count each response separately
                same_item_responses[p1_item]['true' if p1_answer else 'false'] += 1
                same_item_responses[p1_item]['true' if p2_answer else 'false'] += 1
            
            # Skip if we don't have both answers
            if p1_answer is None or p2_answer is None:
                continue
                
            # Calculate correlation: (T,T) or (F,F) count as 1, (T,F) or (F,T) count as -1
            correlation = 1 if p1_answer == p2_answer else -1
            
            pair_counts[(p1_item, p2_item)] += 1
            correlation_sums[(p1_item, p2_item)] += correlation
        
        # Populate the corr_matrix with (numerator, denominator) tuples
        for i, row_item in enumerate(item_values):
            for j, col_item in enumerate(item_values):
                numerator = correlation_sums.get((row_item, col_item), 0)
                denominator = pair_counts.get((row_item, col_item), 0)
                corr_matrix[i][j] = (numerator, denominator)
        
        # Calculate the same-item balance metric
        same_item_balance: Dict[str, float] = {}
        for item, counts in same_item_responses.items():
            total = counts['true'] + counts['false']
            if total == 0:
                same_item_balance[item] = 0.0
            else:
                diff = abs(counts['true'] - counts['false'])
                same_item_balance[item] = 1.0 - diff / total
        
        # Calculate the average balance across all same items
        if same_item_responses:
            avg_same_item_balance = sum(same_item_balance.values()) / len(same_item_balance)
        else:
            avg_same_item_balance = 0.0
        
        return (corr_matrix, item_values,
                avg_same_item_balance, same_item_balance, same_item_responses,
                correlation_sums, pair_counts)
    except Exception as e:
        logger.error(f"Error computing correlation matrix for team {team_id}: {str(e)}", exc_info=True)
        return ([[ (0,0) for _ in range(4) ] for _ in range(4)],
                ['A', 'B', 'X', 'Y'], 0.0, {}, {}, {}, {})

def _compute_success_metrics_optimized(team_id: int, team_rounds: List[Any], team_answers: List[Any], team_obj: Any = None) -> Tuple[List[List[Tuple[int, int]]], List[str], float, float, Dict[Tuple[str, str], int], Dict[Tuple[str, str], int], Dict[str, Dict[str, int]]]:
    """Compute success metrics for new mode using pre-fetched rounds and answers data."""
    try:
        # Use pre-fetched team data to avoid N+1 queries
        if team_obj is None:
            # Fallback to database query if team_obj not provided (backward compatibility)
            team_obj = Teams.query.get(team_id)
            if not team_obj:
                logger.warning(f"Could not find team data for team_id: {team_id}")
                return ([[(0, 0) for _ in range(4)] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, 0.0, {}, {}, {})
        
        db_team = team_obj
        
        # Create a mapping from round_id to the round object for quick access
        round_map = {round_obj.round_id: round_obj for round_obj in team_rounds}
        
        # Group answers by round_id
        answers_by_round: Dict[int, List[Any]] = {}
        for answer in team_answers:
            if answer.question_round_id not in answers_by_round:
                answers_by_round[answer.question_round_id] = []
            answers_by_round[answer.question_round_id].append(answer)
        
        # Initialize success metrics
        item_values = ['A', 'B', 'X', 'Y']
        success_matrix = [[(0, 0) for _ in range(4)] for _ in range(4)]  # (successful_rounds, total_rounds)
        
        # Count pairs for each item combination
        pair_counts: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        success_counts: Dict[Tuple[str, str], int] = {(i, j): 0 for i in item_values for j in item_values}
        
        # Track individual player responses for balance calculation
        player_responses: Dict[str, Dict[str, int]] = {}
        for item in item_values:
            player_responses[item] = {'true': 0, 'false': 0}
        
        total_rounds = 0
        successful_rounds = 0
        
        # Analyze each round that has both player answers
        for round_id, round_answers in answers_by_round.items():
            # Skip if we don't have exactly 2 answers (one from each player)
            if len(round_answers) != 2 or round_id not in round_map:
                continue
                
            round_obj = round_map[round_id]
            p1_item = round_obj.player1_item.value if round_obj.player1_item else None
            p2_item = round_obj.player2_item.value if round_obj.player2_item else None
            
            # Skip if we don't have both items
            if not p1_item or not p2_item:
                continue
                
            # Get player responses using session ID matching to avoid duplicate item bug
            p1_answer = None
            p2_answer = None

            # Match answers by player session ID instead of assigned item to handle duplicate items correctly
            for answer in round_answers:
                if answer.player_session_id == db_team.player1_session_id:
                    p1_answer = answer.response_value
                elif answer.player_session_id == db_team.player2_session_id:
                    p2_answer = answer.response_value
            
            # Skip if we don't have both answers
            if p1_answer is None or p2_answer is None:
                continue
                
            # Track individual player responses for balance calculation
            player_responses[p1_item]['true' if p1_answer else 'false'] += 1
            player_responses[p2_item]['true' if p2_answer else 'false'] += 1
                
            # Apply success rules for new mode
            # Success Rule: {B,Y} combinations require different answers; all others require same answers
            is_by_combination = (p1_item == 'B' and p2_item == 'Y') or (p1_item == 'Y' and p2_item == 'B')
            players_answered_differently = p1_answer != p2_answer
            
            if is_by_combination:
                # B,Y combination: players should answer differently
                is_successful = players_answered_differently
            else:
                # All other combinations: players should answer the same
                is_successful = not players_answered_differently
            
            # Update counts
            total_rounds += 1
            if is_successful:
                successful_rounds += 1
                success_counts[(p1_item, p2_item)] += 1
            
            pair_counts[(p1_item, p2_item)] += 1
        
        # Populate the success matrix with (successful, total) tuples
        for i, row_item in enumerate(item_values):
            for j, col_item in enumerate(item_values):
                successful = success_counts.get((row_item, col_item), 0)
                total = pair_counts.get((row_item, col_item), 0)
                success_matrix[i][j] = (successful, total)
        
        # Calculate overall success rate and normalized score
        overall_success_rate = successful_rounds / total_rounds if total_rounds > 0 else 0.0
        
        # Normalized cumulative score: +1 for success, -1 for failure, divided by total rounds
        score_sum = successful_rounds - (total_rounds - successful_rounds)
        normalized_cumulative_score = score_sum / total_rounds if total_rounds > 0 else 0.0
        
        return (success_matrix, item_values, overall_success_rate, normalized_cumulative_score, success_counts, pair_counts, player_responses)
        
    except Exception as e:
        logger.error(f"Error computing success metrics for team {team_id}: {str(e)}", exc_info=True)
        return ([[(0, 0) for _ in range(4)] for _ in range(4)], ['A', 'B', 'X', 'Y'], 0.0, 0.0, {}, {}, {})

def _calculate_team_statistics_from_data(correlation_result: Tuple) -> Dict[str, Optional[float]]:
    """Calculate ufloat statistics from pre-computed correlation matrix data."""
    try:
        (corr_matrix_tuples, item_values,
         same_item_balance_avg, same_item_balance, same_item_responses,
         correlation_sums, pair_counts) = correlation_result
        
        # Use the same logic as _calculate_team_statistics but with pre-computed data
        # --- Calculate statistics with uncertainties using ufloat ---

        # Stat1: Trace Average Statistic
        sum_of_cii_ufloats: UFloat = ufloat(0, MIN_STD_DEV)
        if len(corr_matrix_tuples) == 4 and all(len(row) == 4 for row in corr_matrix_tuples):
            for i in range(4):
                num, den = corr_matrix_tuples[i][i]
                if den > 0:
                    c_ii_val = num / den
                    c_ii_val = max(-1.0, min(1.0, c_ii_val))
                    stdev_ii = 1 / math.sqrt(den)
                    c_ii_ufloat = ufloat(c_ii_val, stdev_ii)
                else:
                    c_ii_ufloat = ufloat(0, float("inf"))
                sum_of_cii_ufloats += c_ii_ufloat
        
        raw_trace_avg_ufloat = (1 / 4) * sum_of_cii_ufloats
        if raw_trace_avg_ufloat.nominal_value >= 0:
            trace_average_statistic_ufloat = raw_trace_avg_ufloat
        else:
            trace_average_statistic_ufloat = ufloat(-raw_trace_avg_ufloat.nominal_value, raw_trace_avg_ufloat.std_dev)

        # Stat2: CHSH Value Statistic
        chsh_sum_ufloat: UFloat = ufloat(0, MIN_STD_DEV)
        if len(corr_matrix_tuples) == 4 and all(len(row) == 4 for row in corr_matrix_tuples) and all(item in item_values for item in ['A', 'B', 'X', 'Y']):
            try:
                A_idx = item_values.index('A')
                B_idx = item_values.index('B')
                X_idx = item_values.index('X')
                Y_idx = item_values.index('Y')

                terms_indices_coeffs = [
                    (A_idx, X_idx, 1), (A_idx, Y_idx, 1),
                    (B_idx, X_idx, 1), (B_idx, Y_idx, -1),
                    (X_idx, A_idx, 1), (X_idx, B_idx, 1),
                    (Y_idx, A_idx, 1), (Y_idx, B_idx, -1)
                ]

                for r_idx, c_idx, coeff in terms_indices_coeffs:
                    num, den = corr_matrix_tuples[r_idx][c_idx]
                    if den > 0:
                        c_ij_val = num / den
                        c_ij_val = max(-1.0, min(1.0, c_ij_val))
                        stdev_ij = 1 / math.sqrt(den)
                        c_ij_ufloat = ufloat(c_ij_val, stdev_ij)
                    else:
                        c_ij_ufloat = ufloat(0, float("inf"))
                    chsh_sum_ufloat += coeff * c_ij_ufloat
            except ValueError:
                pass
        chsh_value_statistic_ufloat = (1/2) * chsh_sum_ufloat

        # Stat3: Cross-Term Combination Statistic
        cross_term_sum_ufloat: UFloat = ufloat(0, MIN_STD_DEV)
        if all(item in item_values for item in ['A', 'B', 'X', 'Y']):
            term_item_pairs_coeffs = [
                ('A', 'X', 1), ('A', 'Y', 1),
                ('B', 'X', 1), ('B', 'Y', -1)
            ]
            for item1, item2, coeff in term_item_pairs_coeffs:
                M_ij = pair_counts.get((item1, item2), 0) + pair_counts.get((item2, item1), 0)
                if M_ij > 0:
                    N_ij_sum_prod = correlation_sums.get((item1, item2), 0) + correlation_sums.get((item2, item1), 0)
                    t_ij_val = N_ij_sum_prod / M_ij
                    t_ij_val = max(-1.0, min(1.0, t_ij_val))
                    stdev_t_ij = 1 / math.sqrt(M_ij)
                    t_ij_ufloat = ufloat(t_ij_val, stdev_t_ij)
                else:
                    t_ij_ufloat = ufloat(0, float("inf"))
                cross_term_sum_ufloat += coeff * t_ij_ufloat
        cross_term_combination_statistic_ufloat = cross_term_sum_ufloat

        # Sameâ€‘item balance with uncertainty (ufloat)
        same_item_balance_ufloats: List[UFloat] = []
        for item, counts in (same_item_responses or {}).items():
            T_count = counts.get('true', 0)
            F_count = counts.get('false', 0)
            total_tf = T_count + F_count
            if total_tf > 0:
                p_val = T_count / total_tf
                var_p = 1 / total_tf
                p_true = ufloat(p_val, math.sqrt(var_p))
                p_val2 = 2 * p_true - 1
                if p_val2.nominal_value >= 0:
                    abs_p_val2 = p_val2
                else:
                    abs_p_val2 = ufloat(-p_val2.nominal_value, p_val2.std_dev)
                balance_ufloat = 1 - abs_p_val2
                same_item_balance_ufloats.append(balance_ufloat)
            else:
                balance_ufloat = ufloat(0, float("inf"))
                same_item_balance_ufloats.append(balance_ufloat)

        if same_item_balance_ufloats:
            avg_same_item_balance_ufloat = sum(same_item_balance_ufloats) / len(same_item_balance_ufloats)
        else:
            avg_same_item_balance_ufloat = ufloat(0, float("inf"))
        
        return {
            'trace_average_statistic': trace_average_statistic_ufloat.nominal_value,
            'trace_average_statistic_uncertainty': trace_average_statistic_ufloat.std_dev if not math.isinf(trace_average_statistic_ufloat.std_dev) else None,
            'chsh_value_statistic': chsh_value_statistic_ufloat.nominal_value,
            'chsh_value_statistic_uncertainty': chsh_value_statistic_ufloat.std_dev if not math.isinf(chsh_value_statistic_ufloat.std_dev) else None,
            'cross_term_combination_statistic': cross_term_combination_statistic_ufloat.nominal_value,
            'cross_term_combination_statistic_uncertainty': cross_term_combination_statistic_ufloat.std_dev if not math.isinf(cross_term_combination_statistic_ufloat.std_dev) else None,
            'same_item_balance': avg_same_item_balance_ufloat.nominal_value,
            'same_item_balance_uncertainty': (avg_same_item_balance_ufloat.std_dev
                                              if not math.isinf(avg_same_item_balance_ufloat.std_dev) else None)
        }
    except Exception as e:
        logger.error(f"Error calculating team statistics from data: {str(e)}", exc_info=True)
        return {
            'trace_average_statistic': 0.0,
            'trace_average_statistic_uncertainty': None,
            'chsh_value_statistic': 0.0,
            'chsh_value_statistic_uncertainty': None,
            'cross_term_combination_statistic': 0.0,
            'cross_term_combination_statistic_uncertainty': None,
            'same_item_balance': 0.0,
            'same_item_balance_uncertainty': None
        }

def _calculate_success_statistics_from_data(success_result: Tuple) -> Dict[str, Optional[float]]:
    """Calculate success statistics for new mode from pre-computed success metrics data."""
    try:
        (success_matrix_tuples, item_values, overall_success_rate, normalized_cumulative_score, 
         success_counts, pair_counts, player_responses) = success_result
        
        # Calculate success statistics with uncertainties using ufloat
        if overall_success_rate >= 0:
            total_rounds = sum(pair_counts.values())
            if total_rounds > 0:
                success_rate_uncertainty = math.sqrt(overall_success_rate * (1 - overall_success_rate) / total_rounds)
            else:
                success_rate_uncertainty = None
        else:
            success_rate_uncertainty = None
            
        if normalized_cumulative_score is not None:
            total_rounds = sum(pair_counts.values())
            if total_rounds > 0:
                score_uncertainty = 2 / math.sqrt(total_rounds)
            else:
                score_uncertainty = None
        else:
            score_uncertainty = None
            
        # Calculate cross-term combination statistic as average success rate across specific pairs
        cross_term_pairs = [('A', 'X'), ('A', 'Y'), ('B', 'X'), ('B', 'Y')]
        cross_term_success_rates = []
        cross_term_uncertainties = []
        
        for item1, item2 in cross_term_pairs:
            total_pair = pair_counts.get((item1, item2), 0) + pair_counts.get((item2, item1), 0)
            successful_pair = success_counts.get((item1, item2), 0) + success_counts.get((item2, item1), 0)
            
            if total_pair > 0:
                pair_success_rate = successful_pair / total_pair
                pair_uncertainty = math.sqrt(pair_success_rate * (1 - pair_success_rate) / total_pair)
                cross_term_success_rates.append(pair_success_rate)
                cross_term_uncertainties.append(pair_uncertainty)
        
        if cross_term_success_rates:
            cross_term_avg = sum(cross_term_success_rates) / len(cross_term_success_rates)
            if cross_term_uncertainties:
                cross_term_uncertainty = math.sqrt(sum(u**2 for u in cross_term_uncertainties)) / len(cross_term_uncertainties)
            else:
                cross_term_uncertainty = None
        else:
            cross_term_avg = 0.0
            cross_term_uncertainty = None
            
        # Calculate individual player balance for NEW mode
        individual_balances = []
        
        for item, responses in player_responses.items():
            true_count = responses.get('true', 0)
            false_count = responses.get('false', 0)
            total_responses = true_count + false_count
            
            if total_responses > 0:
                balance_ratio = min(true_count, false_count) / total_responses
                item_balance = balance_ratio * 2
                individual_balances.append(item_balance)
        
        if individual_balances:
            same_item_balance_avg = sum(individual_balances) / len(individual_balances)
            if len(individual_balances) > 1:
                variance = sum((b - same_item_balance_avg)**2 for b in individual_balances) / len(individual_balances)
                same_item_balance_uncertainty = math.sqrt(variance)
            else:
                same_item_balance_uncertainty = None
        else:
            same_item_balance_avg = 0.0
            same_item_balance_uncertainty = None
        
        return {
            'trace_average_statistic': overall_success_rate,
            'trace_average_statistic_uncertainty': success_rate_uncertainty,
            'chsh_value_statistic': normalized_cumulative_score,
            'chsh_value_statistic_uncertainty': score_uncertainty,
            'cross_term_combination_statistic': cross_term_avg,
            'cross_term_combination_statistic_uncertainty': cross_term_uncertainty,
            'same_item_balance': same_item_balance_avg,
            'same_item_balance_uncertainty': same_item_balance_uncertainty
        }
    except Exception as e:
        logger.error(f"Error calculating success statistics from data: {str(e)}", exc_info=True)
        return {
            'trace_average_statistic': 0.0,
            'trace_average_statistic_uncertainty': None,
            'chsh_value_statistic': 0.0,
            'chsh_value_statistic_uncertainty': None,
            'cross_term_combination_statistic': 0.0,
            'cross_term_combination_statistic_uncertainty': None,
            'same_item_balance': 0.0,
            'same_item_balance_uncertainty': None
        }

def _process_single_team_optimized(team_id: int, team_name: str, is_active: bool, created_at: Optional[str], current_round: int, player1_sid: Optional[str], player2_sid: Optional[str], team_rounds: List[Any], team_answers: List[Any], team_obj: Any = None) -> Optional[Dict[str, Any]]:
    """
    Process all heavy computation for a single team using pre-fetched data.
    OPTIMIZATION: Uses pre-fetched rounds and answers to avoid database queries.
    """
    try:
        # For active teams, check game progress
        team_info = state.active_teams.get(team_name)
        
        # Mode-specific combo calculation for min_stats_sig
        if state.game_mode == 'new':
            # New mode: Only A,B x X,Y combinations are possible (Player 1: A/B, Player 2: X/Y)
            player1_items = [ItemEnum.A, ItemEnum.B]
            player2_items = [ItemEnum.X, ItemEnum.Y]
            all_combos = [(i1.value, i2.value) for i1 in player1_items for i2 in player2_items]
        else:
            # Classic mode: All combinations possible
            all_combos = [(i1.value, i2.value) for i1 in QUESTION_ITEMS for i2 in QUESTION_ITEMS]
            
        combo_tracker = team_info.get('combo_tracker', {}) if team_info else {}
        effective_combo_repeats = get_effective_combo_repeats(state.game_mode)
        min_stats_sig = all(combo_tracker.get(combo, 0) >= effective_combo_repeats
                        for combo in all_combos) if team_info else False
        
        # Get players list
        players = team_info['players'] if team_info else []

        # Compute hashes for the team using pre-fetched data
        hash1, hash2 = _compute_team_hashes_optimized(team_id, team_rounds, team_answers)
        
        # ALWAYS compute both classic and new statistics for details modal
        # Get correlation matrix and success metrics data using pre-fetched data
        # Pass team_obj to avoid N+1 queries
        correlation_result = _compute_correlation_matrix_optimized(team_id, team_rounds, team_answers, team_obj)
        (corr_matrix_tuples, item_values,
         same_item_balance_avg, same_item_balance, same_item_responses,
         correlation_sums, pair_counts) = correlation_result
        
        success_result = _compute_success_metrics_optimized(team_id, team_rounds, team_answers, team_obj)
        (success_matrix_tuples, success_item_values, overall_success_rate, normalized_cumulative_score, 
         success_counts, success_pair_counts, player_responses) = success_result
        
        # Calculate statistics using pre-computed correlation and success data
        classic_stats = _calculate_team_statistics_from_data(correlation_result)
        new_stats = _calculate_success_statistics_from_data(success_result)
        
        # Determine which matrix and stats to use for the main display based on game mode
        if state.game_mode == 'new':
            display_matrix = success_matrix_tuples
            display_labels = success_item_values
            display_stats = new_stats
        else:
            display_matrix = corr_matrix_tuples
            display_labels = item_values
            display_stats = classic_stats
        
        team_data = {
            'team_name': team_name,
            'team_id': team_id,
            'is_active': is_active,
            'player1_sid': player1_sid,
            'player2_sid': player2_sid,
            'current_round_number': current_round,
            'history_hash1': hash1,
            'history_hash2': hash2,
            'min_stats_sig': min_stats_sig,
            'correlation_matrix': display_matrix,
            'correlation_labels': display_labels,
            'correlation_stats': display_stats,  # Current mode stats for main display
            'classic_stats': classic_stats,      # Always include classic stats for details modal
            'new_stats': new_stats,              # Always include new stats for details modal
            'classic_matrix': corr_matrix_tuples,    # Classic correlation matrix for details modal
            'new_matrix': success_matrix_tuples,     # New success matrix for details modal
            'created_at': created_at,
            'game_mode': state.game_mode  # Include current mode
        }
        
        # Add status field for active teams
        if team_info and 'status' in team_info:
            team_data['status'] = team_info['status']
        elif is_active and len(players) < 2:
            team_data['status'] = 'waiting_pair'
        elif is_active:
            team_data['status'] = 'active'
        else:
            team_data['status'] = 'inactive'
            
        return team_data
    except Exception as e:
        logger.error(f"Error processing team {team_id}: {str(e)}", exc_info=True)
        return None

@selective_cache(_team_process_cache)
def _process_single_team(team_id: int, team_name: str, is_active: bool, created_at: Optional[str], current_round: int, player1_sid: Optional[str], player2_sid: Optional[str]) -> Optional[Dict[str, Any]]:
    """Process all heavy computation for a single team."""
    try:
        # For active teams, check game progress
        team_info = state.active_teams.get(team_name)
        
        # Mode-specific combo calculation for min_stats_sig
        if state.game_mode == 'new':
            # New mode: Only A,B x X,Y combinations are possible (Player 1: A/B, Player 2: X/Y)
            player1_items = [ItemEnum.A, ItemEnum.B]
            player2_items = [ItemEnum.X, ItemEnum.Y]
            all_combos = [(i1.value, i2.value) for i1 in player1_items for i2 in player2_items]
        else:
            # Classic mode: All combinations possible
            all_combos = [(i1.value, i2.value) for i1 in QUESTION_ITEMS for i2 in QUESTION_ITEMS]
            
        combo_tracker = team_info.get('combo_tracker', {}) if team_info else {}
        effective_combo_repeats = get_effective_combo_repeats(state.game_mode)
        min_stats_sig = all(combo_tracker.get(combo, 0) >= effective_combo_repeats
                        for combo in all_combos) if team_info else False
        
        # Get players list
        players = team_info['players'] if team_info else []

        # Compute hashes for the team
        hash1, hash2 = compute_team_hashes(team_name)
        
        # ALWAYS compute both classic and new statistics for details modal
        # Get correlation matrix and success metrics data (cached by team name)
        correlation_result = compute_correlation_matrix(team_name)  # type: ignore
        (corr_matrix_tuples, item_values,
         same_item_balance_avg, same_item_balance, same_item_responses,
         correlation_sums, pair_counts) = correlation_result
        
        success_result = compute_success_metrics(team_name)  # type: ignore
        (success_matrix_tuples, success_item_values, overall_success_rate, normalized_cumulative_score, 
         success_counts, success_pair_counts, player_responses) = success_result
        
        # Calculate statistics (cached by team name)
        classic_stats = _calculate_team_statistics(team_name)
        new_stats = _calculate_success_statistics(team_name)
        
        # Determine which matrix and stats to use for the main display based on game mode
        if state.game_mode == 'new':
            display_matrix = success_matrix_tuples
            display_labels = success_item_values
            display_stats = new_stats
        else:
            display_matrix = corr_matrix_tuples
            display_labels = item_values
            display_stats = classic_stats
        
        team_data = {
            'team_name': team_name,
            'team_id': team_id,
            'is_active': is_active,
            'player1_sid': player1_sid,
            'player2_sid': player2_sid,
            'current_round_number': current_round,
            'history_hash1': hash1,
            'history_hash2': hash2,
            'min_stats_sig': min_stats_sig,
            'correlation_matrix': display_matrix,
            'correlation_labels': display_labels,
            'correlation_stats': display_stats,  # Current mode stats for main display
            'classic_stats': classic_stats,      # Always include classic stats for details modal
            'new_stats': new_stats,              # Always include new stats for details modal
            'classic_matrix': corr_matrix_tuples,    # Classic correlation matrix for details modal
            'new_matrix': success_matrix_tuples,     # New success matrix for details modal
            'created_at': created_at,
            'game_mode': state.game_mode  # Include current mode
        }
        
        # Add status field for active teams
        if team_info and 'status' in team_info:
            team_data['status'] = team_info['status']
        elif is_active and len(players) < 2:
            team_data['status'] = 'waiting_pair'
        elif is_active:
            team_data['status'] = 'active'
        else:
            team_data['status'] = 'inactive'
            
        return team_data
    except Exception as e:
        logger.error(f"Error processing team {team_id}: {str(e)}", exc_info=True)
        return None

def get_all_teams() -> List[Dict[str, Any]]:
    """
    Retrieve and serialize all team data with throttling for performance.
    Returns cached result (even if stale) if called within REFRESH_DELAY_QUICK seconds.
    Thread-safe with minimal lock usage for optimal performance.
    
    OPTIMIZATION: Only locks for cache operations, not expensive computations.
    Uses computation flag to prevent race conditions where multiple threads
    perform duplicate expensive work.
    Uses "stale but usable" cache logic - returns stale data within throttling window.
    """
    global _last_refresh_time, _cached_teams_result, _cached_teams_is_stale, _teams_computation_in_progress
    
    try:
        # First, check cache and computation state under minimal lock
        current_time = time()
        with _safe_dashboard_operation():
            time_since_last_refresh = current_time - _last_refresh_time
            
            # Return cached result (even if stale) if throttling applies
            if time_since_last_refresh < REFRESH_DELAY_QUICK and _cached_teams_result is not None:
                return _cached_teams_result
            
            # If computation in progress, return current cache (avoid duplicate work)
            if _teams_computation_in_progress:
                return _cached_teams_result if _cached_teams_result is not None else []
            
            # Mark computation starting
            _teams_computation_in_progress = True
        
        # === EXPENSIVE OPERATIONS OUTSIDE LOCK ===
        # These are thread-safe and don't need synchronization
        
        # OPTIMIZATION: Bulk fetch all data to prevent N+1 queries
        all_teams = Teams.query.all()
        
        if not all_teams:
            # Update cache under lock and return
            with _safe_dashboard_operation():
                _cached_teams_result = []
                _cached_teams_is_stale = False  # Fresh data is not stale
                _last_refresh_time = time()  # Use fresh timestamp reflecting actual cache completion
                _teams_computation_in_progress = False  # Clear computation flag
            return []
        
        # Extract team IDs for bulk queries
        team_ids = [team.team_id for team in all_teams]
        
        # Bulk fetch all rounds and answers for all teams
        all_rounds = PairQuestionRounds.query.filter(
            PairQuestionRounds.team_id.in_(team_ids)
        ).order_by(PairQuestionRounds.team_id, PairQuestionRounds.timestamp_initiated).all()
        
        all_answers = Answers.query.filter(
            Answers.team_id.in_(team_ids)
        ).order_by(Answers.team_id, Answers.timestamp).all()
        
        # Group data by team_id for efficient lookup
        rounds_by_team = {}
        answers_by_team = {}
        
        for round_obj in all_rounds:
            if round_obj.team_id not in rounds_by_team:
                rounds_by_team[round_obj.team_id] = []
            rounds_by_team[round_obj.team_id].append(round_obj)
        
        for answer in all_answers:
            if answer.team_id not in answers_by_team:
                answers_by_team[answer.team_id] = []
            answers_by_team[answer.team_id].append(answer)
        
        # Process teams using pre-fetched data
        teams_list = []
        
        for team in all_teams:
            # Get active team info from state if available (state reads are atomic)
            team_info = state.active_teams.get(team.team_name)
            
            # Get players from either active state or database
            players = team_info['players'] if team_info else []
            current_round = team_info.get('current_round_number', 0) if team_info else 0
            
            # Get pre-fetched data for this team
            team_rounds = rounds_by_team.get(team.team_id, [])
            team_answers = answers_by_team.get(team.team_id, [])
            
            # Use optimized helper function with pre-fetched data
            team_data = _process_single_team_optimized(
                team.team_id,
                team.team_name,
                team.is_active,
                team.created_at.isoformat() if team.created_at else None,
                current_round,
                players[0] if len(players) > 0 else None,
                players[1] if len(players) > 1 else None,
                team_rounds,
                team_answers,
                team  # Pass team object to avoid N+1 queries
            )
            
            if team_data:
                teams_list.append(team_data)
        
        # === END EXPENSIVE OPERATIONS ===
        
        # Update cache under minimal lock
        with _safe_dashboard_operation():
            _cached_teams_result = teams_list
            _cached_teams_is_stale = False  # Fresh data is not stale
            _last_refresh_time = time()  # Use fresh timestamp reflecting actual cache completion
            _teams_computation_in_progress = False  # Clear computation flag
        
        return teams_list
        
    except Exception as e:
        logger.error(f"Error in get_all_teams: {str(e)}", exc_info=True)
        # Ensure computation flag is cleared even on exception
        with _safe_dashboard_operation():
            _teams_computation_in_progress = False
        return []

def invalidate_team_caches(team_name: str) -> None:
    """
    Selectively mark caches as stale for a specific team only.
    Preserves cached results for all other teams and allows throttling to return stale data.
    Uses "stale but usable" invalidation - marks data as outdated without deleting it.
    Thread-safe operation with proper error handling.
    """
    global _cached_teams_result, _cached_team_metrics, _cached_full_metrics
    global _cached_teams_is_stale, _cached_team_metrics_is_stale, _cached_full_metrics_is_stale
    
    try:
        with _safe_dashboard_operation():
            # Selectively mark team-specific caches as stale (not delete them)
            total_invalidated = 0
            total_invalidated += compute_team_hashes.cache_invalidate_team(team_name)
            total_invalidated += compute_correlation_matrix.cache_invalidate_team(team_name)
            total_invalidated += compute_success_metrics.cache_invalidate_team(team_name)
            total_invalidated += _calculate_team_statistics.cache_invalidate_team(team_name)
            total_invalidated += _calculate_success_statistics.cache_invalidate_team(team_name)
            total_invalidated += _process_single_team.cache_invalidate_team(team_name)
            
            # Mark global throttling caches as stale if they contain this team's data
            # but preserve the cached data for throttling
            if _cached_teams_result is not None:
                # Check if this team is in the cached result
                team_in_cache = any(team.get('team_name') == team_name for team in _cached_teams_result)
                if team_in_cache:
                    _cached_teams_is_stale = True  # Mark as stale instead of clearing
                    logger.debug(f"Marked get_all_teams cache as stale for team {team_name}")
            
            # Mark throttling caches as stale if they contained this team's data
            if _cached_team_metrics is not None and 'cached_teams' in _cached_team_metrics:
                cached_teams = _cached_team_metrics.get('cached_teams', [])
                if any(team.get('team_name') == team_name for team in cached_teams):
                    _cached_team_metrics_is_stale = True  # Mark as stale instead of clearing
                    logger.debug(f"Marked team metrics cache as stale for team {team_name}")
            
            if _cached_full_metrics is not None and 'cached_teams' in _cached_full_metrics:
                cached_teams = _cached_full_metrics.get('cached_teams', [])
                if any(team.get('team_name') == team_name for team in cached_teams):
                    _cached_full_metrics_is_stale = True  # Mark as stale instead of clearing
                    logger.debug(f"Marked full metrics cache as stale for team {team_name}")
            
            logger.debug(f"Selectively marked {total_invalidated} cache entries as stale for team {team_name}")
            
    except Exception as e:
        logger.error(f"Error invalidating team caches for {team_name}: {str(e)}", exc_info=True)

def clear_team_caches() -> None:
    """
    Clear all team-related caches and throttling state to prevent stale data.
    Thread-safe operation with proper error handling to prevent lock issues.
    
    Note: This function now clears ALL caches. For selective invalidation of
    specific teams, use invalidate_team_caches(team_name) instead.
    """
    global _last_refresh_time, _cached_teams_result, _cached_teams_is_stale
    global _last_team_update_time, _last_full_update_time, _cached_team_metrics, _cached_full_metrics
    global _cached_team_metrics_is_stale, _cached_full_metrics_is_stale
    global _teams_computation_in_progress, _team_update_computation_in_progress, _full_update_computation_in_progress
    
    try:
        with _safe_dashboard_operation():
            # Clear all selective caches
            compute_team_hashes.cache_clear()
            compute_correlation_matrix.cache_clear()
            compute_success_metrics.cache_clear()
            _calculate_team_statistics.cache_clear()
            _calculate_success_statistics.cache_clear()
            _process_single_team.cache_clear()
            
            # Clear get_all_teams cache since it depends on caches we just cleared
            _last_refresh_time = 0
            _cached_teams_result = None
            _cached_teams_is_stale = False
            
            # Clear computation flags to prevent stuck state
            _teams_computation_in_progress = False
            _team_update_computation_in_progress = False
            _full_update_computation_in_progress = False
            
            # FIXED: Reset throttling timers when cached teams data is removed to prevent inconsistent state
            # When we remove cached teams data, we must also reset throttling to avoid serving
            # empty teams list with stale metrics in subsequent throttled calls
            if _cached_team_metrics is not None and 'cached_teams' in _cached_team_metrics:
                # Reset team update throttling to ensure consistency
                _last_team_update_time = 0
                _cached_team_metrics = None
                _cached_team_metrics_is_stale = False
                
            if _cached_full_metrics is not None and 'cached_teams' in _cached_full_metrics:
                # Reset full update throttling to ensure consistency
                _last_full_update_time = 0
                _cached_full_metrics = None
                _cached_full_metrics_is_stale = False
            
            logger.debug("Cleared all team caches, computation flags, and reset throttling state to ensure data consistency")
            
        # Perform periodic cleanup of dashboard client data
        # Note: This is outside the main lock to prevent potential deadlocks
        _periodic_cleanup_dashboard_clients()
        
    except Exception as e:
        logger.error(f"Error clearing team caches: {str(e)}", exc_info=True)

def force_clear_all_caches() -> None:
    """
    Force clear ALL caches including throttling state. Use only when data integrity requires it.
    This is more aggressive than clear_team_caches() and should be used sparingly.
    """
    global _last_refresh_time, _cached_teams_result, _cached_teams_is_stale
    global _last_team_update_time, _last_full_update_time, _cached_team_metrics, _cached_full_metrics
    global _cached_team_metrics_is_stale, _cached_full_metrics_is_stale
    global _teams_computation_in_progress, _team_update_computation_in_progress, _full_update_computation_in_progress
    
    try:
        with _safe_dashboard_operation():
            # Clear all selective caches
            compute_team_hashes.cache_clear()
            compute_correlation_matrix.cache_clear()
            compute_success_metrics.cache_clear()
            _calculate_team_statistics.cache_clear()
            _calculate_success_statistics.cache_clear()
            _process_single_team.cache_clear()
            
            # Force clear ALL throttling state
            _last_refresh_time = 0
            _cached_teams_result = None
            _cached_teams_is_stale = False
            _last_team_update_time = 0
            _last_full_update_time = 0
            _cached_team_metrics = None
            _cached_team_metrics_is_stale = False
            _cached_full_metrics = None
            _cached_full_metrics_is_stale = False
            
            # Clear computation flags to prevent stuck state
            _teams_computation_in_progress = False
            _team_update_computation_in_progress = False
            _full_update_computation_in_progress = False
            
            logger.info("Force cleared all caches, computation flags, and throttling state")
            
        _periodic_cleanup_dashboard_clients()
        
    except Exception as e:
        logger.error(f"Error in force_clear_all_caches: {str(e)}", exc_info=True)

def emit_dashboard_team_update() -> None:
    """
    Send team status updates to dashboard clients with throttled metrics calculation.
    Always sends fresh connected_players_count but throttles expensive metrics.
    Uses REFRESH_DELAY_QUICK for frequent team status updates.
    Optimized for minimal lock usage with multiple clients.
    Uses computation flag to prevent race conditions where multiple threads
    perform duplicate expensive work.
    Uses "stale but usable" cache logic - returns stale data within throttling window.
    """
    global _last_team_update_time, _cached_team_metrics, _cached_team_metrics_is_stale, _team_update_computation_in_progress
    
    try:
        # Early exit if no clients
        if not state.dashboard_clients:
            return
        
        # Always calculate connected_players_count fresh (simple operation)
        connected_players_count = len(state.connected_players)
        
        # Check client streaming preferences, throttling, and computation state under minimal lock
        current_time = time()
        with _safe_dashboard_operation():
            streaming_clients = [sid for sid in state.dashboard_clients if dashboard_teams_streaming.get(sid, False)]
            non_streaming_clients = [sid for sid in state.dashboard_clients if not dashboard_teams_streaming.get(sid, False)]
            
            time_since_last_update = current_time - _last_team_update_time
            
            # Check if we can use cached data (even if stale, as long as within throttling window)
            use_cached_data = (time_since_last_update < REFRESH_DELAY_QUICK and 
                             _cached_team_metrics is not None)
            
            if use_cached_data:
                cached_teams = _cached_team_metrics.get('cached_teams', [])
                cached_active_count = _cached_team_metrics.get('active_teams_count', 0)
                cached_ready_count = _cached_team_metrics.get('ready_players_count', 0)
            
            # If computation in progress, use current cache (avoid duplicate work)
            elif _team_update_computation_in_progress:
                if _cached_team_metrics is not None:
                    cached_teams = _cached_team_metrics.get('cached_teams', [])
                    cached_active_count = _cached_team_metrics.get('active_teams_count', 0)
                    cached_ready_count = _cached_team_metrics.get('ready_players_count', 0)
                    use_cached_data = True
                else:
                    # No cache available, will need to compute lightweight metrics outside lock
                    use_cached_data = False
                    need_lightweight_fallback = True
            
            # Mark computation starting if needed  
            elif not use_cached_data and not locals().get('need_lightweight_fallback', False):
                _team_update_computation_in_progress = True
        
        # === EXPENSIVE OPERATIONS OUTSIDE LOCK ===
        
        # Handle lightweight fallback first (computation in progress but no cache)
        if locals().get('need_lightweight_fallback', False):
            # Lightweight metrics computation outside lock
            serialized_teams = []
            active_teams = [team_info for team_info in state.active_teams.values() 
                          if team_info.get('status') in ['active', 'waiting_pair']]
            active_teams_count = len(active_teams)
            ready_players_count = sum(len(team_info.get('players', [])) for team_info in active_teams)
        elif not use_cached_data:
            # Compute fresh data outside lock
            if streaming_clients:
                # Get expensive teams data only if streaming clients need it
                serialized_teams = get_all_teams()  # Already optimized to minimize locks
                active_teams = [team for team in serialized_teams if team.get('is_active', False) or team.get('status') == 'waiting_pair']
                active_teams_count = len(active_teams)
                ready_players_count = sum(
                    (1 if team.get('player1_sid') else 0) + (1 if team.get('player2_sid') else 0)
                    for team in active_teams
                )
            else:
                # Lightweight metrics only
                serialized_teams = []
                active_teams = [team_info for team_info in state.active_teams.values() 
                              if team_info.get('status') in ['active', 'waiting_pair']]
                active_teams_count = len(active_teams)
                ready_players_count = sum(len(team_info.get('players', [])) for team_info in active_teams)
            
            # Update cache under lock
            with _safe_dashboard_operation():
                _cached_team_metrics = {
                    'cached_teams': serialized_teams,
                    'active_teams_count': active_teams_count,
                    'ready_players_count': ready_players_count,
                }
                _cached_team_metrics_is_stale = False  # Fresh data is not stale
                _last_team_update_time = time()  # Use fresh timestamp reflecting actual cache completion
                _team_update_computation_in_progress = False  # Clear computation flag
        else:
            # Use cached data (already retrieved under lock above)
            serialized_teams = cached_teams
            active_teams_count = cached_active_count
            ready_players_count = cached_ready_count
        
        # === SOCKET EMISSIONS OUTSIDE LOCK ===
        # SocketIO handles thread safety internally
        
        # Send full teams data to streaming clients
        if streaming_clients:
            streaming_update_data = {
                'teams': serialized_teams,
                'connected_players_count': connected_players_count,
                'active_teams_count': active_teams_count,
                'ready_players_count': ready_players_count
            }
            
            for sid in streaming_clients:
                socketio.emit('team_status_changed_for_dashboard', streaming_update_data, to=sid)  # type: ignore
        
        # Send metrics-only updates to non-streaming clients
        if non_streaming_clients:
            metrics_update_data = {
                'teams': [],  # Empty teams array for non-streaming clients
                'connected_players_count': connected_players_count,
                'active_teams_count': active_teams_count,
                'ready_players_count': ready_players_count
            }
            
            for sid in non_streaming_clients:
                socketio.emit('team_status_changed_for_dashboard', metrics_update_data, to=sid)  # type: ignore
                
    except Exception as e:
        logger.error(f"Error in emit_dashboard_team_update: {str(e)}", exc_info=True)
        # Ensure computation flag is cleared even on exception
        with _safe_dashboard_operation():
            _team_update_computation_in_progress = False

def emit_dashboard_full_update(client_sid: Optional[str] = None, exclude_sid: Optional[str] = None) -> None:
    """
    Send complete dashboard data to clients with throttled expensive operations.
    Supports targeting specific clients or excluding clients to prevent duplicates.
    Uses REFRESH_DELAY_FULL for expensive operations like database queries.
    Optimized for minimal lock usage with multiple clients.
    Uses computation flag to prevent race conditions where multiple threads
    perform duplicate expensive work.
    Uses "stale but usable" cache logic - returns stale data within throttling window.
    """
    global _last_full_update_time, _cached_full_metrics, _cached_full_metrics_is_stale, _full_update_computation_in_progress
    
    try:
        # Early exit if no clients
        if not state.dashboard_clients:
            return
        
        # Always calculate connected_players_count fresh (simple operation)
        connected_players_count = len(state.connected_players)
        
        # Determine client needs, check throttling, and computation state under minimal lock
        current_time = time()
        with _safe_dashboard_operation():
            if client_sid:
                clients_needing_teams = [client_sid] if dashboard_teams_streaming.get(client_sid, False) else []
            else:
                clients_needing_teams = [sid for sid in state.dashboard_clients 
                                       if dashboard_teams_streaming.get(sid, False) and sid != exclude_sid]
            
            time_since_last_update = current_time - _last_full_update_time
            
            # Check if we can use cached data (even if stale, as long as within throttling window)
            use_cached_data = (time_since_last_update < REFRESH_DELAY_FULL and 
                             _cached_full_metrics is not None)
            
            if use_cached_data:
                cached_teams = _cached_full_metrics.get('cached_teams', [])
                cached_total_answers = _cached_full_metrics.get('total_answers', 0)
                cached_active_count = _cached_full_metrics.get('active_teams_count', 0)
                cached_ready_count = _cached_full_metrics.get('ready_players_count', 0)
            
            # If computation in progress, use current cache (avoid duplicate work)
            elif _full_update_computation_in_progress:
                if _cached_full_metrics is not None:
                    cached_teams = _cached_full_metrics.get('cached_teams', [])
                    cached_total_answers = _cached_full_metrics.get('total_answers', 0)
                    cached_active_count = _cached_full_metrics.get('active_teams_count', 0)
                    cached_ready_count = _cached_full_metrics.get('ready_players_count', 0)
                    use_cached_data = True
                else:
                    # No cache available, will need to compute minimal data outside lock
                    use_cached_data = False
                    need_minimal_fallback = True
            
            # Mark computation starting if needed
            elif not use_cached_data and not locals().get('need_minimal_fallback', False):
                _full_update_computation_in_progress = True
        
        # === EXPENSIVE OPERATIONS OUTSIDE LOCK ===
        
        # Handle minimal fallback first (computation in progress but no cache)
        if locals().get('need_minimal_fallback', False):
            # Minimal data computation outside lock
            all_teams_for_metrics = []
            total_answers = 0  # No database query in fallback mode
            active_teams = [team_info for team_info in state.active_teams.values() 
                          if team_info.get('status') in ['active', 'waiting_pair']]
            active_teams_count = len(active_teams)
            ready_players_count = sum(len(team_info.get('players', [])) for team_info in active_teams)
        elif not use_cached_data:
            # Database query (thread-safe, doesn't need lock)
            with app.app_context():
                total_answers = Answers.query.count()
            
            # Compute fresh data outside lock
            if clients_needing_teams:
                # Get expensive teams data only if clients need it
                all_teams_for_metrics = get_all_teams()  # Already optimized to minimize locks
                active_teams = [team for team in all_teams_for_metrics if team.get('is_active', False) or team.get('status') == 'waiting_pair']
                active_teams_count = len(active_teams)
                ready_players_count = sum(
                    (1 if team.get('player1_sid') else 0) + (1 if team.get('player2_sid') else 0)
                    for team in active_teams
                )
            else:
                # Lightweight metrics only
                all_teams_for_metrics = []
                active_teams = [team_info for team_info in state.active_teams.values() 
                              if team_info.get('status') in ['active', 'waiting_pair']]
                active_teams_count = len(active_teams)
                ready_players_count = sum(len(team_info.get('players', [])) for team_info in active_teams)
            
            # Update cache under lock
            with _safe_dashboard_operation():
                _cached_full_metrics = {
                    'cached_teams': all_teams_for_metrics,
                    'total_answers': total_answers,
                    'active_teams_count': active_teams_count,
                    'ready_players_count': ready_players_count,
                }
                _cached_full_metrics_is_stale = False  # Fresh data is not stale
                _last_full_update_time = time()  # Use fresh timestamp reflecting actual cache completion
                _full_update_computation_in_progress = False  # Clear computation flag
        else:
            # Use cached data (already retrieved under lock above)
            all_teams_for_metrics = cached_teams
            total_answers = cached_total_answers
            active_teams_count = cached_active_count
            ready_players_count = cached_ready_count

        # Prepare base update data (outside lock)
        base_update_data = {
            'total_answers_count': total_answers,
            'connected_players_count': connected_players_count,
            'active_teams_count': active_teams_count,
            'ready_players_count': ready_players_count,
            'game_state': {
                'started': state.game_started,
                'paused': state.game_paused,
                'streaming_enabled': state.answer_stream_enabled,
                'mode': state.game_mode,
                'theme': state.game_theme
            }
        }

        # === SOCKET EMISSIONS OUTSIDE LOCK ===
        # SocketIO handles thread safety internally
        
        if client_sid:
            # For specific client, include teams only if they have streaming enabled
            update_data = base_update_data.copy()
            if dashboard_teams_streaming.get(client_sid, False):
                update_data['teams'] = all_teams_for_metrics
            else:
                update_data['teams'] = []
            socketio.emit('dashboard_update', update_data, to=client_sid)  # type: ignore
        else:
            # For all clients, send appropriate data based on their preferences
            for dash_sid in state.dashboard_clients:
                # Skip excluded client to prevent duplicate updates
                if exclude_sid and dash_sid == exclude_sid:
                    continue
                    
                update_data = base_update_data.copy()
                if dashboard_teams_streaming.get(dash_sid, False):
                    update_data['teams'] = all_teams_for_metrics
                else:
                    update_data['teams'] = []
                socketio.emit('dashboard_update', update_data, to=dash_sid)  # type: ignore
    except Exception as e:
        logger.error(f"Error in emit_dashboard_full_update: {str(e)}", exc_info=True)
        # Ensure computation flag is cleared even on exception
        with _safe_dashboard_operation():
            _full_update_computation_in_progress = False

@socketio.on('dashboard_join')
def on_dashboard_join(data: Optional[Dict[str, Any]] = None, callback: Optional[Any] = None) -> None:
    try:
        sid = request.sid  # type: ignore
        
        # Add to dashboard clients with teams streaming disabled by default (only for new clients)
        state.dashboard_clients.add(sid)
        dashboard_last_activity[sid] = time()
        if sid not in dashboard_teams_streaming:
            dashboard_teams_streaming[sid] = False  # Teams streaming off by default for new clients
        logger.info(f"Dashboard client connected: {sid}")
        
        # Notify OTHER dashboard clients about the new connection (exclude the joining client to prevent duplicates)
        emit_dashboard_full_update(exclude_sid=sid)
        
        # Prepare update data for this specific client, respecting their streaming preference
        with app.app_context():
            total_answers = Answers.query.count()
            
        # Only get expensive teams data if this client has streaming enabled
        if dashboard_teams_streaming.get(sid, False):
            all_teams_for_metrics = get_all_teams()
            # Calculate metrics from the teams data we just fetched
            active_teams = [team for team in all_teams_for_metrics if team.get('is_active', False) or team.get('status') == 'waiting_pair']
            active_teams_count = len(active_teams)
            ready_players_count = sum(
                (1 if team.get('player1_sid') else 0) + (1 if team.get('player2_sid') else 0)
                for team in active_teams
            )
        else:
            # Calculate lightweight metrics from state without expensive team processing
            all_teams_for_metrics = []
            active_teams = [team_info for team_info in state.active_teams.values() 
                          if team_info.get('status') in ['active', 'waiting_pair']]
            active_teams_count = len(active_teams)
            ready_players_count = sum(len(team_info.get('players', [])) for team_info in active_teams)
        
        update_data = {
            'teams': all_teams_for_metrics if dashboard_teams_streaming.get(sid, False) else [],  # Respect client's streaming preference
            'total_answers_count': total_answers,
            'connected_players_count': len(state.connected_players),
            'active_teams_count': active_teams_count,  # Always send metrics
            'ready_players_count': ready_players_count,  # Always send metrics
            'game_state': {
                'started': state.game_started,
                'streaming_enabled': state.answer_stream_enabled,
                'mode': state.game_mode  # Include current game mode
            }
        }
        
        # If callback provided, use it to return data directly
        if callback:
            callback(update_data)
        else:
            # Send update to the joining client only once
            socketio.emit('dashboard_update', update_data, to=sid)  # type: ignore
    except Exception as e:
        logger.error(f"Error in on_dashboard_join: {str(e)}", exc_info=True)
        emit('error', {'message': 'An error occurred while joining the dashboard'})  # type: ignore

@socketio.on('start_game')
def on_start_game(data: Optional[Dict[str, Any]] = None) -> None:
    try:
        if request.sid in state.dashboard_clients:  # type: ignore
            state.game_started = True
            # Notify teams and dashboard that game has started
            for team_name, team_info in state.active_teams.items():
                if len(team_info['players']) == 2:  # Only notify paired teams
                    socketio.emit('game_start', {'game_started': True}, to=team_name)  # type: ignore
            
            # Notify dashboard
            for dashboard_sid in state.dashboard_clients:
                socketio.emit('game_started', to=dashboard_sid)  # type: ignore
                
            # Notify all clients about game state change
            socketio.emit('game_state_changed', {'game_started': True})  # type: ignore
                
            # Start first round for all paired teams
            for team_name, team_info in state.active_teams.items():
                if len(team_info['players']) == 2: # If team is paired
                    start_new_round_for_pair(team_name)
    except Exception as e:
        logger.error(f"Error in on_start_game: {str(e)}", exc_info=True)
        emit('error', {'message': 'An error occurred while starting the game'})  # type: ignore

@socketio.on('pause_game')
def on_pause_game() -> None:
    try:
        if request.sid not in state.dashboard_clients:  # type: ignore
            emit('error', {'message': 'Unauthorized: Not a dashboard client'})  # type: ignore
            return

        state.game_paused = not state.game_paused  # Toggle pause state
        pause_status = "paused" if state.game_paused else "resumed"
        logger.info(f"Game {pause_status} by {request.sid}")  # type: ignore

        # Notify all clients about pause state change
        for team_name in state.active_teams.keys():
            socketio.emit('game_state_update', {
                'paused': state.game_paused
            }, to=team_name)  # type: ignore

        # Update dashboard state
        emit_dashboard_full_update()

    except Exception as e:
        logger.error(f"Error in on_pause_game: {str(e)}", exc_info=True)
        emit('error', {'message': 'An error occurred while toggling game pause'})  # type: ignore

def handle_dashboard_disconnect(sid: str) -> None:
    """Handle disconnect logic for dashboard clients with proper cleanup and error handling"""
    try:
        if sid in state.dashboard_clients:
            state.dashboard_clients.remove(sid)
            
        # Clean up all dashboard client tracking data atomically
        _atomic_client_update(sid, remove=True)
        
    except Exception as e:
        logger.error(f"Error in handle_dashboard_disconnect: {str(e)}", exc_info=True)

@socketio.on('restart_game')
def on_restart_game() -> None:
    try:
        logger.info(f"Received restart_game from {request.sid}")  # type: ignore
        if request.sid not in state.dashboard_clients:  # type: ignore
            emit('error', {'message': 'Unauthorized: Not a dashboard client'})  # type: ignore
            emit('game_reset_complete', to=request.sid)  # type: ignore
            return

        # First update game state to prevent new answers during reset
        state.game_started = False
        # logger.debug("Set game_started=False")
        
        # Even if there are no active teams, clear the database
        try:
            # Clear database entries within a transaction
            db.session.begin_nested()  # Create savepoint
            PairQuestionRounds.query.delete()
            Answers.query.delete()
            db.session.commit()
            # Force clear all caches after successful database commit since this is a complete reset
            force_clear_all_caches()
        except Exception as db_error:
            db.session.rollback()
            logger.error(f"Database error during game reset: {str(db_error)}", exc_info=True)
            emit('error', {'message': 'Database error during reset'})  # type: ignore
            emit('game_reset_complete', to=request.sid)  # type: ignore
            return

        # If no active teams, still complete the reset successfully
        if not state.active_teams:
            socketio.emit('game_state_changed', {'game_started': False})  # type: ignore
            emit_dashboard_full_update()
            emit('game_reset_complete', to=request.sid)  # type: ignore
            return
        
        # Reset team state after successful database clear
        for team_name, team_info in state.active_teams.items():
            if team_info:  # Validate team info exists
                team_info['current_round_number'] = 0
                team_info['current_db_round_id'] = None
                team_info['answered_current_round'] = {}
                team_info['combo_tracker'] = {}
        
        # Notify all teams about the reset
        for team_name in state.active_teams.keys():
            socketio.emit('game_reset', to=team_name)  # type: ignore
        
        # Ensure all clients are notified of the state change
        socketio.emit('game_state_changed', {'game_started': False})  # type: ignore
        
        # Update dashboard with reset state
        emit_dashboard_full_update()

        # Notify dashboard clients that reset is complete
        logger.info("Emitting game_reset_complete to all dashboard clients")
        for dash_sid in state.dashboard_clients:
            socketio.emit('game_reset_complete', to=dash_sid)  # type: ignore
            
    except Exception as e:
        logger.error(f"Error in on_restart_game: {str(e)}", exc_info=True)
        emit('error', {'message': 'An error occurred while restarting the game'})  # type: ignore

@app.route('/api/dashboard/data', methods=['GET'])
def get_dashboard_data():
    try:
        # Get all answers ordered by timestamp
        with app.app_context():
            all_answers = Answers.query.order_by(Answers.timestamp.asc()).all()
        
        answers_data = []
        for ans in all_answers:
            # Get team name for each answer
            team = Teams.query.get(ans.team_id)
            team_name = team.team_name if team else "Unknown Team"
            
            answers_data.append({
                'answer_id': ans.answer_id,
                'team_id': ans.team_id,
                'team_name': team_name,
                'player_session_id': ans.player_session_id,
                'question_round_id': ans.question_round_id,
                'assigned_item': ans.assigned_item.value,
                'response_value': ans.response_value,
                'timestamp': ans.timestamp.isoformat()
            })
        
        return jsonify({'answers': answers_data}), 200
    except Exception as e:
        logger.error(f"Error in get_dashboard_data: {str(e)}", exc_info=True)
        return jsonify({'error': 'An error occurred while retrieving dashboard data'}), 500

@app.route('/download', methods=['GET'])
def download_csv():
    try:
        # Get all answers ordered by timestamp
        with app.app_context():
            all_answers = Answers.query.order_by(Answers.timestamp.asc()).all()
        
        # Create CSV content in memory
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Write CSV header
        writer.writerow(['Timestamp', 'Team Name', 'Team ID', 'Player ID', 'Round ID', 'Question Item (A/B/X/Y)', 'Answer (True/False)'])
        
        # Write data rows
        for ans in all_answers:
            # Get team name for each answer
            team = Teams.query.get(ans.team_id)
            team_name = team.team_name if team else "Unknown Team"
            
            writer.writerow([
                ans.timestamp.strftime('%m/%d/%Y, %I:%M:%S %p'),  # Format timestamp like JavaScript toLocaleString()
                team_name,
                ans.team_id,
                ans.player_session_id,
                ans.question_round_id,
                ans.assigned_item.value,
                ans.response_value
            ])
        
        # Get the CSV content
        csv_content = output.getvalue()
        output.close()
        
        # Create response with appropriate headers for CSV download
        response = Response(
            csv_content,
            mimetype='text/csv',
            headers={
                'Content-Disposition': 'attachment; filename=chsh-game-data.csv'
            }
        )
        
        return response
        
    except Exception as e:
        logger.error(f"Error in download_csv: {str(e)}", exc_info=True)
        return Response(
            "An error occurred while generating the CSV file",
            status=500,
            mimetype='text/plain'
        )

# Disconnect handler is now consolidated in team_management.py
# The handle_dashboard_disconnect function is called from there
